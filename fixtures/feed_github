<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="https://githubengineering.com/atom.xml" rel="self" type="application/atom+xml" /><link href="https://githubengineering.com/" rel="alternate" type="text/html" /><updated>2017-08-24T22:09:25+00:00</updated><id>https://githubengineering.com/</id><title type="html">GitHub Engineering</title><subtitle>The Blog of the GitHub Engineering Team</subtitle><author><name>GitHub Engineering</name></author><entry><title type="html">Kubernetes at GitHub</title><link href="https://githubengineering.com/kubernetes-at-github/" rel="alternate" type="text/html" title="Kubernetes at GitHub" /><published>2017-08-16T00:00:00+00:00</published><updated>2017-08-16T00:00:00+00:00</updated><id>https://githubengineering.com/kubernetes-at-github</id><content type="html" xml:base="https://githubengineering.com/kubernetes-at-github/">&lt;p&gt;Over the last year, GitHub has gradually evolved the infrastructure that runs the Ruby on Rails application responsible for &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;api.github.com&lt;/code&gt;. We reached a big milestone recently: all web and API requests are served by containers running in &lt;a href=&quot;https://github.com/kubernetes/kubernetes/&quot;&gt;Kubernetes&lt;/a&gt; clusters deployed on our &lt;a href=&quot;https://githubengineering.com/githubs-metal-cloud/&quot;&gt;metal cloud&lt;/a&gt;. Moving a critical application to Kubernetes was a fun challenge, and we’re excited to share some of what we’ve learned with you today.&lt;/p&gt;&lt;h2 id=&quot;why-change&quot;&gt;Why change?&lt;/h2&gt;&lt;p&gt;Before this move, our main Ruby on Rails application (we call it &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;) was configured a lot like it was eight years ago: &lt;a href=&quot;https://github.com/blog/517-unicorn&quot;&gt;Unicorn&lt;/a&gt; processes managed by a Ruby process manager called &lt;a href=&quot;http://godrb.com/&quot;&gt;God&lt;/a&gt; running on Puppet-managed servers. Similarly, our &lt;a href=&quot;https://githubengineering.com/deploying-branches-to-github-com/&quot;&gt;chatops deployment&lt;/a&gt; worked a lot like it did when it was first introduced: Capistrano established SSH connections to each frontend server, then &lt;a href=&quot;https://github.com/blog/470-deployment-script-spring-cleaning&quot;&gt;updated the code in place&lt;/a&gt; and restarted application processes. When peak request load exceeded available frontend CPU capacity, GitHub Site Reliability Engineers would &lt;a href=&quot;https://githubengineering.com/githubs-metal-cloud/&quot;&gt;provision additional capacity&lt;/a&gt; and add it to the pool of active frontend servers.&lt;/p&gt;&lt;div style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;Previous unicorn service design&quot; src=&quot;/images/kubernetes-at-github/before.png&quot; /&gt;&lt;/div&gt;&lt;p&gt;While our basic production approach didn’t change much in those years, GitHub itself changed a lot: new features, larger software communities, more GitHubbers on staff, and way more requests per second. As we grew, this approach began to exhibit new problems. Many teams wanted to extract the functionality they were responsible for from this large application into a smaller service that could run and be deployed independently. As the number of services we ran increased, the SRE team began supporting similar configurations for dozens of other applications, increasing the percentage of our time we spent on server maintenance, provisioning, and other work not directly related to improving the overall GitHub experience. New services took days, weeks, or months to deploy depending on their complexity and the SRE team’s availability. Over time, it became clear that this approach did not provide our engineers the flexibility they needed to continue building a world-class service. Our engineers needed a self-service platform they could use to experiment, deploy, and scale new services. We also needed that same platform to fit the needs of our core Ruby on Rails application so that engineers and/or robots could respond to changes in demand by allocating additional compute resources in seconds instead of hours, days, or longer.&lt;/p&gt;&lt;p&gt;In response to those needs, the SRE, Platform, and Developer Experience teams began a joint project that led us from an initial evaluation of container orchestration platforms to where we are today: deploying the code that powers &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;api.github.com&lt;/code&gt; to Kubernetes clusters dozens of times per day. This post aims to provide a high-level overview of the work involved in that journey.&lt;/p&gt;&lt;h2 id=&quot;why-kubernetes&quot;&gt;Why Kubernetes?&lt;/h2&gt;&lt;p&gt;As a part of evaluating the existing landscape of “platform as a service” tools, we took a closer look at Kubernetes, a project from Google that described itself at the time as &lt;em&gt;an open-source system for automating deployment, scaling, and management of containerized applications&lt;/em&gt;. Several qualities of Kubernetes stood out from the other platforms we evaluated: the vibrant open source community supporting the project, the first run experience (which allowed us to deploy a small cluster and an application in the first few hours of our initial experiment), and a wealth of information available about the &lt;a href=&quot;http://queue.acm.org/detail.cfm?id=2898444&quot;&gt;experience&lt;/a&gt; that motivated its design.&lt;/p&gt;&lt;p&gt;These experiments quickly grew in scope: a small project was assembled to build a Kubernetes cluster and deployment tooling in support of an upcoming hack week to gain some practical experience with the platform. Our experience with this project as well as the feedback from engineers who used it was overwhelmingly positive. It was time to expand our experiments, so we started planning a larger rollout.&lt;/p&gt;&lt;h2 id=&quot;why-start-with-githubgithub&quot;&gt;Why start with &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;?&lt;/h2&gt;&lt;p&gt;At the earliest stages of this project, we made a deliberate decision to target the migration of a critical workload: &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;. Many factors contributed to this decision, but a few stood out:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;We knew that the deep knowledge of this application throughout GitHub would be useful during the process of migration.&lt;/li&gt; &lt;li&gt;We needed self-service capacity expansion tooling to handle continued growth.&lt;/li&gt; &lt;li&gt;We wanted to make sure the habits and patterns we developed were suitable for large applications as well as smaller services.&lt;/li&gt; &lt;li&gt;We wanted to better insulate the app from differences between development, staging, production, enterprise, and other environments.&lt;/li&gt; &lt;li&gt;We knew that migrating a critical, high-visibility workload would encourage further Kubernetes adoption at GitHub.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Given the critical nature of the workload we chose to migrate, we needed to build a high level of operational confidence before serving any production traffic.&lt;/p&gt;&lt;h2 id=&quot;rapid-iteration-and-confidence-building-with-a-review-lab&quot;&gt;Rapid iteration and confidence building with a review lab&lt;/h2&gt;&lt;p&gt;As a part of this migration, we designed, prototyped, and validated a replacement for the service currently provided by our frontend servers using Kubernetes primitives like Pods, Deployments, and Services. Some validation of this new design could be performed by running &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;’s existing test suites in a container rather than on a server configured similarly to frontend servers, but we also needed to observe how this container behaved as a part of a larger set of Kubernetes resources. It quickly became clear that an environment that supported exploratory testing of the combination of Kubernetes and the services we intended to run would be necessary during the validation phase.&lt;/p&gt;&lt;p&gt;Around the same time, we observed that our existing patterns for exploratory testing of &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt; pull requests had begun to show signs of growing pains. As the rate of deploys increased along with the number of engineers working on the project, so did the utilization of the several &lt;a href=&quot;https://githubengineering.com/deploying-branches-to-github-com/#deploy-environments&quot;&gt;additional deploy environments&lt;/a&gt; used as a part of the process of validating a pull request to &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;. The small number of fully-featured deploy environments were usually booked solid during peak working hours, which slowed the process of deploying a pull request. Engineers frequently requested the ability to test more of the various production subsystems on “branch lab.” While branch lab allowed concurrent deployment from many engineers, it only started a single Unicorn process for each, which meant it was only useful when testing API and UI changes. These needs overlapped substantially enough for us to combine the projects and start work on a new Kubernetes-powered deployment environment for &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt; called “review lab.”&lt;/p&gt;&lt;p&gt;In the process of building review lab, we shipped a handful of sub-projects, each of which could likely be covered in their own blog post. Along the way, we shipped:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;A Kubernetes cluster running in an AWS VPC managed using a combination of &lt;a href=&quot;https://github.com/hashicorp/terraform&quot;&gt;Terraform&lt;/a&gt; and &lt;a href=&quot;https://github.com/kubernetes/kops&quot;&gt;kops&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A set of Bash integration tests that exercise ephemeral Kubernetes clusters, used heavily in the beginning of the project to gain confidence in Kubernetes.&lt;/li&gt; &lt;li&gt;A Dockerfile for &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Enhancements to our internal CI platform to support building and publishing containers to a container registry.&lt;/li&gt; &lt;li&gt;YAML representations of 50+ Kubernetes resources, checked into &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Enhancements to our internal deployment application to support deploying Kubernetes resources from a repository into a Kubernetes namespace, as well as the creation of Kubernetes secrets from our internal secret store.&lt;/li&gt; &lt;li&gt;A service that combines haproxy and consul-template to route traffic from Unicorn pods to the existing services that publish service information there.&lt;/li&gt; &lt;li&gt;A service that reads Kubernetes events and sends abnormal ones to our internal error tracking system.&lt;/li&gt; &lt;li&gt;A &lt;a href=&quot;https://github.com/bhuga/hubot-chatops-rpc&quot;&gt;chatops-rpc&lt;/a&gt;-compatible service called &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-me&lt;/code&gt; that exposes a limited set of &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; commands to users via chat.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The end result is a chat-based interface for creating an isolated deployment of GitHub for any pull request. Once a pull request passed all required CI jobs, a user can deploy their pull request to review lab like so:&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars0.githubusercontent.com/jnewland?v=3&amp;amp;s=22&quot; alt=&quot;jnewland&quot; srcset=&quot;https://avatars0.githubusercontent.com/jnewland?v=3&amp;amp;s=22 1x, https://avatars0.githubusercontent.com/jnewland?v=3&amp;amp;s=44 2x, https://avatars0.githubusercontent.com/jnewland?v=3&amp;amp;s=66 3x, https://avatars0.githubusercontent.com/jnewland?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;jnewland&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .deploy https://github.com/github/github/pull/4815162342 to review-lab &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; @jnewland's review-lab deployment of github/add-pre-stop-hook (00cafefe) is done! (12 ConfigMaps, 17 Deployments, 1 Ingress, 1 Namespace, 6 Secrets, and 23 Services)(77.62s) your lab is available at https://jnewland.review-lab.github.com &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;p&gt;Like branch lab before it, labs are cleaned up one day after their last deploy. As each lab is created in its own Kubernetes namespace, cleanup is as simple as deleting the namespace, which our deployment system performs automatically when necessary.&lt;/p&gt;&lt;p&gt;Review lab was a successful project with a number of positive outcomes. Before making this environment generally available to engineers, it served as an essential proving ground and prototyping environment for our Kubernetes cluster design as well as the design and configuration of the Kubernetes resources that now describe the &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt; Unicorn workload. After release, it exposed a large number of engineers to a new style of deployment, helping us build confidence via feedback from interested engineers as well as continued use from engineers who didn’t notice any change. And just recently, we observed some engineers on our High Availability team use review lab to experiment with the interaction between Unicorn and the behavior of a new experimental subsystem by deploying it to a shared lab. We’re extremely pleased with the way that this environment empowers engineers to experiment and solve problems in a self-service manner.&lt;/p&gt;&lt;div style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;Deploys per day to branch lab and review lab&quot; src=&quot;/images/kubernetes-at-github/deploys.png&quot; /&gt;&lt;/div&gt;&lt;h2 id=&quot;kubernetes-on-metal&quot;&gt;Kubernetes on Metal&lt;/h2&gt;&lt;p&gt;With review lab shipped, our attention shifted to &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com&lt;/code&gt;. To satisfy the performance and reliability requirements of our flagship service - which depends on low-latency access to other data services - we needed to build out Kubernetes infrastructure that supported the &lt;a href=&quot;https://githubengineering.com/githubs-metal-cloud/&quot;&gt;metal cloud&lt;/a&gt; we run in our physical data centers and POPs. Again, nearly a dozen subprojects were involved in this effort:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;A timely and thorough post about &lt;a href=&quot;https://jvns.ca/blog/2016/12/22/container-networking/&quot;&gt;container networking&lt;/a&gt; helped us select the &lt;a href=&quot;https://www.projectcalico.org/&quot;&gt;Calico&lt;/a&gt; network provider, which provided the out-of-the box functionality we needed to ship a cluster quickly in &lt;code class=&quot;highlighter-rouge&quot;&gt;ipip&lt;/code&gt; mode while giving us the flexibility to explore peering with our network infrastructure later.&lt;/li&gt; &lt;li&gt;Following no less than a dozen reads of @kelseyhightower’s indispensable &lt;a href=&quot;https://github.com/kelseyhightower/kubernetes-the-hard-way&quot;&gt;Kubernetes the hard way&lt;/a&gt;, we assembled a handful of manually provisioned servers into a temporary Kubernetes cluster that passed the same set of integration tests we used to exercise our AWS clusters.&lt;/li&gt; &lt;li&gt;We built a small tool to generate the CA and configuration necessary for each cluster in a format that could be consumed by our internal Puppet and secret systems.&lt;/li&gt; &lt;li&gt;We Puppetized the configuration of two instance roles - Kubernetes nodes and Kubernetes apiservers - in a fashion that allows a user to provide the name of an already-configured cluster to join at provision time.&lt;/li&gt; &lt;li&gt;We built a small Go service to consume container logs, append metadata in key/value format to each line, and send them to the hosts’ local syslog endpoint.&lt;/li&gt; &lt;li&gt;We enhanced &lt;a href=&quot;https://githubengineering.com/introducing-glb/&quot;&gt;GLB&lt;/a&gt;, our internal load balancing service, to support Kubernetes NodePort Services.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The combination of all of this hard work resulted in a cluster that passed our internal acceptance tests. Given that, we were fairly confident that the same set of inputs (the Kubernetes resources in use by review lab), the same set of data (the network services review lab connected to over a VPN), and same tools would create a similar result. In less than a week’s time - much of which was spent on internal communication and sequencing in the event the migration had significant impact - we were able to migrate this entire workload from a Kubernetes cluster running on AWS to one running inside one of our data centers.&lt;/p&gt;&lt;h2 id=&quot;raising-the-confidence-bar&quot;&gt;Raising the confidence bar&lt;/h2&gt;&lt;p&gt;With a successful and repeatable pattern for assembling Kubernetes clusters on our metal cloud, it was time to build confidence in the ability of our Unicorn deployment to replace the pool of current frontend servers. At GitHub, it is common practice for engineers and their teams to validate new functionality by creating a &lt;a href=&quot;https://github.com/jnunemaker/flipper&quot;&gt;Flipper&lt;/a&gt; feature and then opting into it as soon as it is viable to do so. After enhancing our deployment system to deploy a new set of Kubernetes resources to a &lt;code class=&quot;highlighter-rouge&quot;&gt;github-production&lt;/code&gt; namespace in parallel with our existing production servers and enhancing GLB to support routing staff requests to a different backend based on a Flipper-influenced cookie, we allowed staff to opt-in to the experimental Kubernetes backend with a button in our &lt;a href=&quot;https://github.com/blog/1252-how-we-keep-github-fast&quot;&gt;mission control bar&lt;/a&gt;:&lt;/p&gt;&lt;div style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;Staff UI for opting-in to Kubernetes-powered infrastructure&quot; src=&quot;/images/kubernetes-at-github/button.png&quot; /&gt;&lt;/div&gt;&lt;p&gt;The load from internal users helped us find problems, fix bugs, and start getting comfortable with Kubernetes in production. During this period, we worked to increase our confidence by simulating procedures we anticipated performing in the future, writing runbooks, and performing failure tests. We also routed small amounts of production traffic to this cluster to confirm our assumptions about performance and reliability under load, starting with 100 requests per second and expanding later to 10% of the requests to &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;api.github.com&lt;/code&gt;. With several of these simulations under our belt, we paused briefly to re-evaluate the risk of a full migration.&lt;/p&gt;&lt;div style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;Kubernetes unicorn service design&quot; src=&quot;/images/kubernetes-at-github/after.png&quot; /&gt;&lt;/div&gt;&lt;h2 id=&quot;cluster-groups&quot;&gt;Cluster Groups&lt;/h2&gt;&lt;p&gt;Several of our failure tests produced results we didn’t expect. Particularly, a test that simulated the failure of a single apiserver node disrupted the cluster in a way that negatively impacted the availability of running workloads. Investigations into the results of these tests did not produce conclusive results, but helped us identify that the disruption was likely related to an interaction between the various clients that connect to the Kubernetes apiserver (like &lt;code class=&quot;highlighter-rouge&quot;&gt;calico-agent&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;kubelet&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-proxy&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-controller-manager&lt;/code&gt;) and our internal load balancer’s behavior during an apiserver node failure. Given that we had observed a Kubernetes cluster degrade in a way that might disrupt service, we started looking at running our flagship application on multiple clusters in each site and automating the process of diverting requests away from a unhealthy cluster to the other healthy ones.&lt;/p&gt;&lt;p&gt;Similar work was already on our roadmap to support deploying this application into multiple independently-operated sites, and other positive trade-offs of this approach - including presenting a viable story for low-disruption cluster upgrades and associating clusters with existing failure domains like shared network and power devices - influenced us to go down this route. We eventually settled on a design that uses our deployment system’s support for deploying to multiple “partitions” and enhanced it to support cluster-specific configuration via a custom Kubernetes resource annotation, forgoing the existing federation solutions for an approach that allowed us to use the business logic already present in our deployment system.&lt;/p&gt;&lt;h2 id=&quot;from-10-to-100&quot;&gt;From 10% to 100%&lt;/h2&gt;&lt;p&gt;With Cluster Groups in place, we gradually converted frontend servers into Kubernetes nodes and increased the percentage of traffic routed to Kubernetes. Alongside a number of other responsible engineering groups, we completed the frontend transition in just over a month while keeping performance and error rates within our targets.&lt;/p&gt;&lt;div style=&quot;text-align:center&quot;&gt;&lt;img alt=&quot;Percentage of web traffic served by cluster&quot; src=&quot;/images/kubernetes-at-github/rollout.png&quot; /&gt;&lt;/div&gt;&lt;p&gt;During this migration, we encountered an issue that persists to this day: during times of high load and/or high rates of container churn, some of our Kubernetes nodes will kernel panic and reboot. While we’re not satisfied with this situation and are continuing to investigate it with high priority, we’re happy that Kubernetes is able to route around these failures automatically and continue serving traffic within our target error bounds. We’ve performed a handful of failure tests that simulated kernel panics with &lt;code class=&quot;highlighter-rouge&quot;&gt;echo c &amp;gt; /proc/sysrq-trigger&lt;/code&gt; and have found this to be a useful addition to our failure testing patterns.&lt;/p&gt;&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;&lt;p&gt;We’re inspired by our experience migrating this application to Kubernetes, and are looking forward to migrating more soon. While scope of our first migration was intentionally limited to stateless workloads, we’re excited about experimenting with patterns for running stateful services on Kubernetes.&lt;/p&gt;&lt;p&gt;During the last phase of this project, we also shipped a workflow for deploying new applications and services into a similar group of Kubernetes clusters. Over the last several months, engineers have already deployed dozens of applications to this cluster. Each of these applications would have previously required configuration management and provisioning support from SREs. With a self-service application provisioning workflow in place, SRE can devote more of our time to delivering infrastructure products to the rest of the engineering organization in support of our best practices, building toward a faster and more resilient GitHub experience for everyone.&lt;/p&gt;&lt;h2 id=&quot;thanks&quot;&gt;Thanks&lt;/h2&gt;&lt;p&gt;We’d like to extend our deep thanks to the entire Kubernetes team for their software, words, and guidance along the way. I’d also like to thank the following GitHubbers for their incredible work on this project: @samlambert, @jssjr, @keithduncan, @jbarnette, @sophaskins, @aaronbbrown, @rhettg, @bbasata, and @gamefiend.&lt;/p&gt;&lt;h2 id=&quot;come-work-with-us&quot;&gt;Come work with us!&lt;/h2&gt;&lt;p&gt;Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us. Apply &lt;a href=&quot;https://boards.greenhouse.io/github/jobs/788701&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;</content><author><name>Jesse Newland</name></author><summary type="html">Over the last year, GitHub has gradually evolved the infrastructure that runs the Ruby on Rails application responsible for github.com and api.github.com. We reached a big milestone recently: all web and API requests are served by containers running in Kubernetes clusters deployed on our metal cloud. Moving a critical application to Kubernetes was a fun challenge, and we’re excited to share some of what we’ve learned with you today.</summary></entry><entry><title type="html">Topic Suggestions for Millions of Repositories</title><link href="https://githubengineering.com/topics/" rel="alternate" type="text/html" title="Topic Suggestions for Millions of Repositories" /><published>2017-07-31T00:00:00+00:00</published><updated>2017-07-31T00:00:00+00:00</updated><id>https://githubengineering.com/topics</id><content type="html" xml:base="https://githubengineering.com/topics/">&lt;p&gt;We recently launched &lt;a href=&quot;https://help.github.com/articles/about-topics/&quot;&gt;Topics&lt;/a&gt;, a new feature that lets you tag your repositories with descriptive words or phrases, making it easy to discover projects and explore GitHub.com. Topic suggestions on public repositories, provides a quick way to add tags to repositories.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/7935808/22560946/9de115ca-e933-11e6-96ff-87f15e4cf989.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;&lt;p&gt;These suggestions are the result of recent data science work at GitHub. We applied concepts from text mining, natural language processing (NLP), and machine learning to build a topic extraction framework.&lt;/p&gt;&lt;h1 id=&quot;solving-the-cold-start-problem-for-topic-suggestions&quot;&gt;Solving the cold-start problem for topic suggestions&lt;/h1&gt;&lt;p&gt;Because Topics is a brand new concept at GitHub, we started with no cues from users on what defined a topic and what type of topics they would typically add to their repositories. Given our focus on improving discoverability, internally we defined Topics as any “&lt;em&gt;word or phrase that roughly describes the purpose of a repository and the type of content it encapsulates&lt;/em&gt;”. These can be words such as “data science”, “nlp”, “scikit-learn”, “clustering algorithm”, “jekyll plugin”, “css template”, or “python”.&lt;/p&gt;&lt;p&gt;While no tag or label-type feature existed prior to the start of this project, we did have a rich set of textual information to start from. At its core, GitHub is a platform for sharing software with other people, and some of the data typically found in a repository provides information to humans rather than instructions for computers. Repository names, descriptions, and READMEs are text that communicate functionality, use case, and features to human readers. That’s where we started.&lt;/p&gt;&lt;h1 id=&quot;repo-topix-our-topics-extraction-framework&quot;&gt;Repo-Topix: our topics extraction framework&lt;/h1&gt;&lt;p&gt;We developed a topic extraction framework, called &lt;strong&gt;repo-topix&lt;/strong&gt;, to learn from the human-readable text that users provide in repo names, descriptions, and READMEs written by developers about their projects by incorporating methods from text mining, NLP, and supervised machine learning. At a high level, repo-topix does three things:&lt;/p&gt;&lt;ol&gt; &lt;li&gt;Generates candidate topics from natural language text by incorporating data from millions of other repositories&lt;/li&gt; &lt;li&gt;Selects the best topics from the set of candidates&lt;/li&gt; &lt;li&gt;Finds similarities and relationships in topics to facilitate discoverability&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Below, we describe each step of the repo-topix framework in greater technical detail.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/7935808/22806042/69e4d892-eedd-11e6-8c2a-142eaa1de519.png&quot; alt=&quot;flow&quot; /&gt;&lt;/p&gt;&lt;h2 id=&quot;generating-candidate-topics-from-natural-language-text&quot;&gt;Generating candidate topics from natural language text&lt;/h2&gt;&lt;h3 id=&quot;preprocessing-of-readmes&quot;&gt;Preprocessing of READMEs&lt;/h3&gt;&lt;p&gt;While README files within GitHub.com tend to be formatted using Markdown and reStructuredText with fairly lightweight formatting, there are certain sections such as code blocks, tables, and image links that are not useful for topic suggestions. For example, month names from within a table would not be useful to a user.&lt;/p&gt;&lt;p&gt;To extract text sections of interest, we developed a heuristics-based README tagger that marks sections in the README file as relevant or non-relevant. This simple tagger uses common formatting cues such as indentation, spacing, and use of backticks to determine “noise sections” and “valid text sections”. The use of a grammar-based parser was unnecessary as we only care about useful text sections and regard everything else in a README as noise.&lt;/p&gt;&lt;p&gt;Once we extract text sections of interest, we perform basic cleanup to remove file extensions, HTML tags, paths, and hosts from further processing, as these are more distracting than useful. Finally, the remaining text gets segmented into coarse-grained units using punctuation marks as well as README section markers such as contiguous hash symbols.&lt;/p&gt;&lt;h3 id=&quot;finding-candidate-topics&quot;&gt;Finding candidate topics&lt;/h3&gt;&lt;p&gt;We use the cleaned text from the previous step to generate candidate topics by eliminating low-information words and breaking the remaining text into strings of one or multiple consecutive words, called n-grams. Like any text, our sources contain many words that are so common that they do not contain distinguishing information. Called stop words, these typically include determiners like “is”, “the”, “are”, conjunctions like “and,”, “but”, and “yet”, and so on. Given our specialized domain, we created a custom stop word list that included words that are practically ubiquitous in our source text; for example, “push”, “pull”, “software”, “tool”, “var”, “val”, and “package.” The custom stop word list provides an efficient way to finding potential topics, as we simply take the resulting phrases left between eliminated words. For example, &lt;em&gt;“this open source software is used for web scraping and search”&lt;/em&gt; produces three candidate topics: 1. “open source,” 2. “web scraping,” 3. “search”. This process eliminates the need for brute-force n-gram generation which could end up producing a large number of n-grams depending the length of the README files being processed. After testing internally among GitHub staff, we found that n-grams made up of many words tended to be too specific (e.g. “machine-learning-tutorial-part-1-intro”), so we limit candidate topics to n-grams of size 4 or less.&lt;/p&gt;&lt;h2 id=&quot;selecting-best-topics&quot;&gt;Selecting best topics&lt;/h2&gt;&lt;h3 id=&quot;eliminating-low-quality-topics&quot;&gt;Eliminating low quality topics&lt;/h3&gt;&lt;p&gt;While some of the generated word units in the previous step would be meaningful as topics, some could also be plain noise. We have a few strategies for pruning noise and unpromising candidate topics. The first is simply to eliminate phrases with low frequency counts. For example, if we had the following candidates with their corresponding counts, we could eliminate some of the low frequency topics:&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;machine learning tutorial part 1 (1)machine learning (5)beginners data science course (1)topic modeling (3)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From the above, we could easily eliminate topics that don’t satisfy a minimum frequency count threshold. However, this method doesn’t prune out topics with unwanted grammatical structure or word composition. For example, words and phrases like “great”, “cool”, “running slowly”, “performing operations”, “install database” and “&lt;a href=&quot;https://github.com/kavgan&quot;&gt;@kavgan&lt;/a&gt;” (a GitHub handle) are not great topics for a repository. To aggressively prune out these keywords, we developed a supervised logistic regression model, trained to classify a topic as “good” (positive) or “bad” (negative). We call this the keyword filtering model.We manually gathered about 300 training examples balanced across the positive (good topics) and negative (bad topics) categories. Because of this manual process with no input from users, our training data is actually fairly small. While it’s possible to learn from the actual words that make up a topic when you have a very large training set, with limited training data we used features that draw on the meta-information of the training examples so that our model does not just memorize specific words. For instance, one of the features we used was the &lt;a href=&quot;http://www.myenglishpages.com/site_php_files/grammar-lesson-parts-of-speech.php&quot;&gt;part-of-speech&lt;/a&gt; usage within topics. If the model learns that single word &lt;em&gt;verbs&lt;/em&gt; are often considered bad topics, the next time it sees such an occurrence, it would help eliminate such words from further consideration. Other features we used were occurrence of user names, n-gram size of a phrase, length of a phrase, and numeric content within a phrase. Our classifier is tuned for high recall in order to keep as many phrases as possible and prune obviously incorrect ones.&lt;/p&gt;&lt;p&gt;With time, we plan to include feedback from users to update the keyword filtering model. For example, highly accepted topics can serve as positive training examples and highly rejected topics can either be used as stop words or used as negative examples in our model. We believe that this incremental update would help weed out uninteresting topics from the suggestions list.&lt;/p&gt;&lt;h3 id=&quot;scoring-candidate-topics&quot;&gt;Scoring candidate topics&lt;/h3&gt;&lt;p&gt;Instead of treating all remaining candidate topics with equal importance, we rank the candidates by score to return only the top-N promising topics instead of a large list of arbitrary topics. We experimented with several scoring schemes. The first scoring approach measures the average strength of association of words in a phrase using &lt;a href=&quot;https://en.wikipedia.org/wiki/Pointwise_mutual_information&quot;&gt;pointwise mutual information&lt;/a&gt; (PMI) weighted by the frequency count of the phrases. The second approach we tried uses the average &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.1424&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;tf-idf&lt;/a&gt; scores of individual words in a phrase weighted by the phrase frequency (if it’s more than one word long) and n-gram size.&lt;/p&gt;&lt;p&gt;We found that the first scoring strategy favored topics that were unique in nature because of the way PMI works when data is fairly sparse: unique phrases tend to get very high scores. While some highly unique phrases can be interesting, some unique phrases can just be typos or even code snippets that were not formatted as code. The second approach favored phrases that were less unique and relatively frequent. We ended up using the tf-idf based scoring as it gave us a good balance between uniqueness of a topic and relevance of a topic to a repository. While our tf (term frequency) scoring is based on local counts, our idf (inverse document frequency) weighting is based on a large dictionary of idf scores built using the unstructured content from millions of public READMEs. The idf weights essentially tell us how common or unique a term is globally. The intuition is that the more common a term, the less information it carries and should thus have a lower weight. For example, in the GitHub domain, the term “application” is much more common than terms such as “machine”, “learning”, or “assignment” and this is clearly reflected by their idf weights as shown below:&lt;/p&gt;&lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;word&lt;/th&gt; &lt;th&gt;idf&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;application&lt;/td&gt; &lt;td&gt;4.300&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;machine&lt;/td&gt; &lt;td&gt;7.169&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;learning&lt;/td&gt; &lt;td&gt;7.818&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;assignment&lt;/td&gt; &lt;td&gt;8.480&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;If a phrase has many words with low idf weighting, then its overall score should be lower compared to a phrase with more significant words - this is the intuition behind our tf-idf scoring strategy. As an example, assuming that the normalized tf of each word above is 0.5, the average tf-idf score for “machine-learning-application” would be &lt;code class=&quot;highlighter-rouge&quot;&gt;3.21&lt;/code&gt; and the average tf-idf score for “machine-learning-assignment” would be &lt;code class=&quot;highlighter-rouge&quot;&gt;3.91&lt;/code&gt;. The former has a lower score because the term “application” is more ubiquitous and has a lower idf score than the term “assignment”.&lt;/p&gt;&lt;p&gt;In addition to the base tf-idf scoring, we are also experimenting with some additional ideas such as boosting scores of those phrases that tend to occur earlier in a document and aren’t unique to a few repositories. These minor tweaks are subject to change based on our internal evaluation.&lt;/p&gt;&lt;h2 id=&quot;discovering-similar-topics&quot;&gt;Discovering similar topics&lt;/h2&gt;&lt;h3 id=&quot;canonicalizing-topics-to-address-character-level-topic-differences-and-inflections&quot;&gt;Canonicalizing topics to address character level topic differences and inflections&lt;/h3&gt;&lt;p&gt;Because different users can express similar phrases in different ways, the generated topics can also vary from repository to repository. For example, we have commonly seen these variation of topics with different repositories:&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;neural-networkneural-networksneuralnetworkneuralnetworks&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;topic-modelstopic-modellingtopic-modelingtopicmodeltopicmodeling&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;machinelearning-algorithmsmachine-learning-algorithm&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To keep topic suggestions fairly consistent, we use a dictionary to canonicalize suggested topics. Instead of suggesting the original topics discovered, we suggest a canonicalized version of the topic if present in our dictionary. This in-house dictionary was built using all non-canonicalized topics across public repositories. The non-canonicalized topics give us cues as to which topics are most commonly used and which ones can be grouped together as being equivalent. We currently use a combination of &lt;a href=&quot;https://en.wikipedia.org/wiki/Edit_distance&quot;&gt;edit-distance&lt;/a&gt;, &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html&quot;&gt;stemming&lt;/a&gt;, and word-level &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard similarity&lt;/a&gt; to group similar topics together. Jaccard similarity in our case estimates how similar two phrases are by comparing members of two sets to see which members are shared and which are distinct. With this, phrases that share many words can be grouped together.&lt;/p&gt;&lt;h3 id=&quot;near-similar-topics&quot;&gt;Near similar topics&lt;/h3&gt;&lt;p&gt;While it’s possible to suggest all top-scoring topics, some topics may be fairly repetitive, and the set of topics returned may not provide enough variety for labeling a repository. For example, the following top-scoring topics (from an actual repository), while valid and meaningful, are not interesting and lack variety as it captures different granularity of similar topics:&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;machine learningdeep learninggeneral-purpose machine learningmachine learning librarymachine learning algorithmsdistributed machine learningmachine learning frameworkdeep learning librarysupport vector machinelinear regression&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We use a greedy topic selection strategy that starts with the highest-scoring topic. If the topic is similar to other lower-scoring topics, the lower-scoring topics are dropped from consideration. We repeat this process iteratively using the next highest-scoring topic until all candidate topics have been accounted for. For the example above, the final set of topics returned to the user would be as follows:&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;machine learningdeep learningsupport vector machinelinear regression&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We use word-level Jaccard similarity when computing similarity between phrases, because it’s known to work well for short phrases. It also produces a score between 0-1, making it easy to set thresholds.&lt;/p&gt;&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;&lt;p&gt;As topic labels were not available during the development of repo-topix, we needed to get a rough approximation of how well the suggested topics describe a repository. For this rough approximation, we used the description text for repositories since descriptions often provide insights into the function of a repository. If indeed the auto-suggested topics are not completely arbitrary, there should be some amount of overlap between suggested topics and the description field. For this evaluation, we computed &lt;a href=&quot;http://83.212.103.151/~mkalochristianakis/techNotes/ipromo/rougen5.pdf&quot;&gt;ROUGE-1&lt;/a&gt; precision and recall. ROUGE is an n-gram overlap metric that counts the number of overlapping units between a system summary (suggested topics in our case) and a gold standard summary (description in our case). We performed this evaluation on roughly 127,000 public repositories with fairly long descriptions. These are our most recent results:&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Repos with no topic suggestions: ~28%Average ROUGE-1 Recall: 0.259Average ROUGE-1 Precision: 0.372F-Measure: 0.306&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The ROUGE recall above tells us quantitatively how much of the description is being captured by topic suggestions and precision tells us what proportion of the suggestion words are words that are also in the description. Based on the results we see that there is some overlap as expected. We’re not looking for perfect overlap, but some level of overlap after disregarding all stop words.&lt;/p&gt;&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;&lt;p&gt;Our topics extraction framework is capable of discovering promising topics for any public repository on GitHub.com. Instead of applying heavy NLP and complex parsing algorithms within our framework (e.g. grammar-based markdown parsing, dependency parsing, chunking, lemmatization), we focused on using lightweight methods that would easily scale as GitHub.com’s repository base grows over the years. For many of our tasks, we leverage the volume in available data to build out reusable dictionaries such as the IDF dictionary, which was built using all public README files, a custom stop-word list, and a canonicalization dictionary for topics. While we currently depend on the presence of README files to generate suggestions, in the future we hope to make suggestions by looking at any available content within a repository. Most of the core topics extraction code was developed using Java and Python within the &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; framework.&lt;/p&gt;&lt;h1 id=&quot;future-plans&quot;&gt;Future plans&lt;/h1&gt;&lt;p&gt;Our plan for the near future is to evaluate the usage of suggested topics as well as manually created topics to continuously improve suggestions shown to users. Some of the rejected topics could feed into our topics extraction framework as stop words or as negative examples to our keyword filtering model. Highly accepted topics could add positive examples to our keyword filtering model and also provide lessons on the type of topics that users care about. This would provide cues as to what type of “meta-information” users add to their repositories in addition to the descriptive terms found within README files. We also plan to explore topic suggestions on private repositories and with GitHub Enterprise in a way that fully respects privacy concerns and eliminates certain data dependencies.&lt;/p&gt;&lt;p&gt;Beyond these near term goals, our vision for using topics is to build an ever-evolving GitHub knowledge graph containing concepts and mapping how they relate to each other and to the code, people, and projects on GitHub.&lt;/p&gt;&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;&lt;p&gt;These are references to some of the libraries that we used:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/&quot;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/mllib/&quot;&gt;https://spark.apache.org/mllib/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://tartarus.org/martin/PorterStemmer/&quot;&gt;https://tartarus.org/martin/PorterStemmer/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://nlp.stanford.edu/software/tagger.shtml&quot;&gt;http://nlp.stanford.edu/software/tagger.shtml&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h1 id=&quot;work-with-us&quot;&gt;Work with us&lt;/h1&gt;&lt;p&gt;Want to work on interesting problems like code analysis, social network analysis, recommendations engine and improving search relevance? Apply &lt;a href=&quot;https://boards.greenhouse.io/github/jobs/668602#.WVpzq4jys2w&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;&lt;h1 id=&quot;topics-data-team&quot;&gt;Topics Data Team&lt;/h1&gt;&lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://www.kavita-ganesan.com&quot;&gt;Kavita Ganesan&lt;/a&gt;, &lt;a href=&quot;https://github.com/kavgan/&quot;&gt;@kavgan&lt;/a&gt;, Senior Data Scientist - Machine Learning&lt;/li&gt; &lt;li&gt;Rafer Hazen, &lt;a href=&quot;https://github.com/rafer/&quot;&gt;@rafer&lt;/a&gt;, Engineering Manager - Data Engineering&lt;/li&gt; &lt;li&gt;Frances Zlotnick, &lt;a href=&quot;https://github.com/franniez/&quot;&gt;@franniez&lt;/a&gt;, Senior Data Scientist - Analytics&lt;/li&gt;&lt;/ul&gt;</content><author><name>Kavita Ganesan</name></author><summary type="html">We recently launched Topics, a new feature that lets you tag your repositories with descriptive words or phrases, making it easy to discover projects and explore GitHub.com. Topic suggestions on public repositories, provides a quick way to add tags to repositories.</summary></entry><entry><title type="html">Soft U2F</title><link href="https://githubengineering.com/soft-u2f/" rel="alternate" type="text/html" title="Soft U2F" /><published>2017-07-20T00:00:00+00:00</published><updated>2017-07-20T00:00:00+00:00</updated><id>https://githubengineering.com/soft-u2f</id><content type="html" xml:base="https://githubengineering.com/soft-u2f/">&lt;p&gt;In an effort to increase the adoption of &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_2nd_Factor&quot;&gt;FIDO U2F&lt;/a&gt; second factor authentication, we’re releasing &lt;a href=&quot;http://github.com/github/SoftU2F&quot;&gt;Soft U2F&lt;/a&gt;: a software-based U2F authenticator for macOS.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/1144197/28530169-198b3ca2-7050-11e7-952b-21358b0fa0b0.gif&quot; alt=&quot;Soft U2F Demo&quot; /&gt;&lt;/p&gt;&lt;p&gt;We’ve long been interested in promoting better user security through two-factor authentication on GitHub.com. Initially, we &lt;a href=&quot;https://github.com/blog/1614-two-factor-authentication&quot;&gt;added support&lt;/a&gt; for &lt;a href=&quot;https://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm&quot;&gt;TOTP&lt;/a&gt;-based 2FA. A few years later, we &lt;a href=&quot;https://github.com/blog/2071-github-supports-universal-2nd-factor-authentication&quot;&gt;added support&lt;/a&gt; for FIDO U2F. U2F provides a better user experience, while overcoming several security shortcomings of TOTP. Unfortunately, U2F adoption has been low, presumably due to the need to purchase a physical device.&lt;/p&gt;&lt;p&gt;In order to lower the barrier to using U2F, we’ve developed a software-based U2F authenticator for macOS: &lt;a href=&quot;http://github.com/github/SoftU2F&quot;&gt;Soft U2F&lt;/a&gt;. Authenticators are normally USB devices that communicate over the HID protocol. By emulating a HID device, Soft U2F is able to communicate with your U2F-enabled browser, and by extension, any websites implementing U2F.&lt;/p&gt;&lt;p&gt;The Soft U2F installer can be downloaded &lt;a href=&quot;https://github.com/github/SoftU2F/releases&quot;&gt;here&lt;/a&gt; and the source code can be found &lt;a href=&quot;http://github.com/github/SoftU2F&quot;&gt;here&lt;/a&gt;. Contributions to the project are welcome.&lt;/p&gt;&lt;h3 id=&quot;security-considerations-of-hardware-vs-software-key-storage&quot;&gt;Security considerations of hardware vs. software key storage&lt;/h3&gt;&lt;p&gt;A USB authenticator stores key material in hardware, whereas Soft U2F stores its keys in the macOS Keychain. There is an argument to be made that it is more secure to store keys in hardware since malware running on your computer can access the contents of your Keychain but cannot export the contents of a hardware authenticator. On the other hand, malware can also access your browser’s cookies and has full access to all authenticated website sessions, regardless of where U2F keys are stored.&lt;/p&gt;&lt;p&gt;In the case of malware installed on your computer, one meaningful difference between hardware and software key storage for U2F is the duration of the compromise. With hardware key storage, you are only compromised while the malware is running on your computer. With software key storage, you could continue to be compromised, even after the malware has been removed.&lt;/p&gt;&lt;p&gt;Some people may decide the attack scenario above is worth the usability tradeoff of hardware key storage. But, for many, the security of software-based U2F is sufficient and helps to mitigate against many common attacks such as password dumps, brute force attacks, and phishing related exploits.&lt;/p&gt;</content><author><name>mastahyeti</name></author><summary type="html">In an effort to increase the adoption of FIDO U2F second factor authentication, we’re releasing Soft U2F: a software-based U2F authenticator for macOS.</summary></entry><entry><title type="html">MySQL infrastructure testing automation at GitHub</title><link href="https://githubengineering.com/mysql-testing-automation-at-github/" rel="alternate" type="text/html" title="MySQL infrastructure testing automation at GitHub" /><published>2017-07-06T00:00:00+00:00</published><updated>2017-07-06T00:00:00+00:00</updated><id>https://githubengineering.com/mysql-testing-automation-at-github</id><content type="html" xml:base="https://githubengineering.com/mysql-testing-automation-at-github/">&lt;p&gt;Our MySQL infrastructure is a critical component to GitHub. MySQL serves GitHub.com, GitHub’s API, authentication and more. Every &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; request touches MySQL in some way. We are tasked with keeping the data available, and maintaining its integrity. Even while our MySQL clusters serve traffic, we need to be able to perform tasks such as heavy duty cleanups, ad-hoc updates, online schema migrations, cluster topology refactoring, pooling and load balancing and more. We have the infrastructure to automate away such operations; in this post we share a few examples of how we build trust in our infrastructure through continuous testing. It is essentially how we sleep well at night.&lt;/p&gt;&lt;h2 id=&quot;backups&quot;&gt;Backups&lt;/h2&gt;&lt;p&gt;It is incredibly important to take backups of your data. If you are not taking backups of your database, it is likely a matter of time before this will become an issue. Percona &lt;a href=&quot;https://www.percona.com/software/mysql-database/percona-xtrabackup&quot;&gt;Xtrabackup&lt;/a&gt; is the tool we have been using for issuing full backups for our MySQL databases. If there is data that we need to be certain is saved, we have a server that is backing up the data.&lt;/p&gt;&lt;p&gt;In addition to the full binary backups, we run logical backups several times a day. These backups allow our engineers to get a copy of recent data. There are times that they would like a complete set of data from a table so they can test an index change on a production sized table or see data from a certain point of time. Hubot allows us to restore a backed up table and will ping us when the table is ready to use.&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=22&quot; alt=&quot;tomkrouper&quot; srcset=&quot;https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=22 1x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=44 2x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=66 3x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;tomkrouper&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .mysql backup-list locations &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; &lt;pre&gt;+-----------+------------+---------------+---------------------+---------------------+----------------------------------------------+| Backup ID | Table Name | Donor Host | Backup Start | Backup End | File Name |+-----------+------------+---------------+---------------------+---------------------+----------------------------------------------+| 1699494 | locations | db-mysql-0903 | 2017-07-01 22:09:17 | 2017-07-01 22:09:17 | backup-mycluster-locations-1498593122.sql.gz || 1699133 | locations | db-mysql-0903 | 2017-07-01 16:11:37 | 2017-07-01 16:11:39 | backup-mycluster-locations-1498571521.sql.gz || 1698772 | locations | db-mysql-0903 | 2017-07-01 10:09:21 | 2017-07-01 10:09:22 | backup-mycluster-locations-1498549921.sql.gz || 1698411 | locations | db-mysql-0903 | 2017-07-01 04:12:32 | 2017-07-01 04:12:32 | backup-mycluster-locations-1498528321.sql.gz || 1698050 | locations | db-mysql-0903 | 2017-06-30 22:18:23 | 2017-06-30 22:18:23 | backup-mycluster-locations-1498506721.sql.gz || ...| 1262253 | locations | db-mysql-0088 | 2016-08-01 01:58:51 | 2016-08-01 01:58:54 | backup-mycluster-locations-1470034801.sql.gz || 1064984 | locations | db-mysql-0088 | 2016-04-04 13:07:40 | 2016-04-04 13:07:43 | backup-mycluster-locations-1459494001.sql.gz |+-----------+------------+---------------+---------------------+---------------------+----------------------------------------------+ &lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=22&quot; alt=&quot;tomkrouper&quot; srcset=&quot;https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=22 1x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=44 2x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=66 3x, https://avatars2.githubusercontent.com/tomkrouper?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;tomkrouper&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .mysql restore 1699133 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; A restore job has been created for the backup job 1699133. You will be notified in #database-ops when the restore is complete. &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; @tomkrouper: the locations table has been restored as locations_2017_07_01_16_11 in the restores database on db-mysql-0482 &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;p&gt;The data is loaded onto a non-production database which is accessible to the engineer requesting the restore.&lt;/p&gt;&lt;p&gt;The last way we keep a “backup” of data around is we use &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.6/en/replication-delayed.html&quot;&gt;delayed replicas&lt;/a&gt;. This is less of a backup and more of a safeguard. For each production cluster we have a host that has replication delayed by 4 hours. If a query is run that shouldn’t have, we can run &lt;code class=&quot;highlighter-rouge&quot;&gt;mysql panic&lt;/code&gt; in chatops. This will cause all of our delayed replicas to stop replication immediately. This will also page the on-call DBA. From there we can use delayed replica to verify there is an issue, and then fast forward the binary logs to the point right before the error. We can then restore this data to the master, thus recovering data to that point.&lt;/p&gt;&lt;p&gt;Backups are great, however they are worthless if some unknown or uncaught error occurs corrupting the backup. A benefit of having a script to restore backups is it allows us to automate the verification of backups via cron. We have set up a dedicated host for each cluster that runs a restore of the latest backup. This ensures that the backup ran correctly and that we are able to retrieve the data from the backup.&lt;/p&gt;&lt;p&gt;Depending on dataset size, we run several restores per day. Restored servers are expected to join the replication stream and to be able to catch up with replication. This tests not only that we took a restorable backup, but also that we correctly identified the point in time at which it was taken and can further apply changes from that point in time. We are alerted if anything goes wrong in the restore process.&lt;/p&gt;&lt;p&gt;We furthermore track the time the restore takes, so we have a good idea of how long it will take to build a new replica or restore in cases of emergency.&lt;/p&gt;&lt;p&gt;The following is an output from an automated restore process, written by Hubot in our robots chat room.&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; gh-mysql-backup-restore: db-mysql-0752: restore_log.id = 4447 &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Determining backup to restore for cluster 'prodcluster'. &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Enabling maintenance mode &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Setting orchestrator downtime &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Disabling Puppet &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Stopping MySQL &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Removing MySQL files &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Running gh-xtrabackup-restore &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Restore file: xtrabackup-notify-2017-07-02_0000.xbstream &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Running gh-xtrabackup-prepare &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Starting MySQL &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Update file ownership &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Upgrade MySQL &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Stopping MySQL &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Starting MySQL &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Backup Host: db-mysql-0034 &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Setting up replication &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Starting replication &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Replication catch-up &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Restore complete (replication running) &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Enabling Puppet &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Disabling maintenance mode &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Setting orchestrator downtime &lt;br /&gt; gh-mysql-backup-restore: db-mysql-0752: Restore process complete. &lt;br /&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;p&gt;One thing we use backups for is adding a new replica to an existing set of MySQL servers. We will initiate the build of a new server, and once we are notified it is ready, we can start a restore of the latest backup for that particular cluster. We have a script in place that runs all of the restore commands that we would otherwise have to do by hand. Our automated restore system essentially uses the same script. This simplifies the system build process and allows us to have a host up and running with a handful of chat commands opposed to dozens of manual processes. Shown below is a restore kicked manually in chat:&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars3.githubusercontent.com/jessbreckenridge?v=3&amp;amp;s=22&quot; alt=&quot;jessbreckenridge&quot; srcset=&quot;https://avatars3.githubusercontent.com/jessbreckenridge?v=3&amp;amp;s=22 1x, https://avatars3.githubusercontent.com/jessbreckenridge?v=3&amp;amp;s=44 2x, https://avatars3.githubusercontent.com/jessbreckenridge?v=3&amp;amp;s=66 3x, https://avatars3.githubusercontent.com/jessbreckenridge?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;jessbreckenridge&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .mysql backup-restore -H db-mysql-0007 -o -r magic_word=daily_rotating_word &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Determining backup to restore for cluster 'mycluster'. &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: restore_log.id = 4449 &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Enabling maintenance mode &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Setting orchestrator downtime &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Disabling Puppet &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Stopping MySQL &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Removing MySQL files &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Running gh-xtrabackup-restore &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Restore file: xtrabackup-mycluster-2017-07-02_0015.xbstream &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Running gh-xtrabackup-prepare &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Update file ownership &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting MySQL &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Upgrade MySQL &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Stopping MySQL &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting MySQL &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Setting up replication &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting replication &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Backup Host: db-mysql-0201 &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Replication catch-up &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Replication behind by 4589 seconds, waiting 1800 seconds before next check. &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Restore complete (replication running) &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Enabling puppet &lt;br /&gt; @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Disabling maintenance mode &lt;br /&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;h2 id=&quot;failovers&quot;&gt;Failovers&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://githubengineering.com/orchestrator-github/&quot;&gt;We use orchestrator&lt;/a&gt; to perform automated failovers for masters and intermediate masters. We expect &lt;code class=&quot;highlighter-rouge&quot;&gt;orchestrator&lt;/code&gt; to correctly detect master failure, designate a replica for promotion, heal the topology under said designated replica, make the promotion. We expect VIPs to change, pools to change, clients to reconnect, &lt;code class=&quot;highlighter-rouge&quot;&gt;puppet&lt;/code&gt; to run essential components on promoted master, and more. A failover is a complex task that touches many aspects of our infrastructure.&lt;/p&gt;&lt;p&gt;To build trust in our failovers we set up a &lt;em&gt;production-like&lt;/em&gt;, test cluster, and we continuously crash it to observe failovers.&lt;/p&gt;&lt;p&gt;The &lt;em&gt;production-like&lt;/em&gt; cluster is a replication setup that is identical in all aspects to our production clusters: types of hardware, operating systems, MySQL versions, network environments, VIP, &lt;code class=&quot;highlighter-rouge&quot;&gt;puppet&lt;/code&gt; configurations, &lt;a href=&quot;https://githubengineering.com/context-aware-mysql-pools-via-haproxy/&quot;&gt;haproxy setup&lt;/a&gt;, etc. The only thing different to this cluster is that it doesn’t send/receive production traffic.&lt;/p&gt;&lt;p&gt;We emulate a write load on the test cluster, while avoiding replication lag. The write load is not too heavy, but has queries that are intentionally contending to write on same datasets. This isn’t too interesting in normal times, but proves to be useful upon failovers, as we will shortly describe.&lt;/p&gt;&lt;p&gt;Our test cluster has representative servers from three data centers. We would &lt;em&gt;like&lt;/em&gt; the failover to promote a replacement replica from within the same data center. We would &lt;em&gt;like&lt;/em&gt; to be able to salvage as many replicas as possible under such constraint. We &lt;em&gt;require&lt;/em&gt; that both apply whenever possible. &lt;code class=&quot;highlighter-rouge&quot;&gt;orchestrator&lt;/code&gt; has no prior assumption on the topology; it must react on whatever the state was at time of the crash.&lt;/p&gt;&lt;p&gt;We, however, are interested in creating complex and varying scenarios for failovers. Our failover testing script prepares the grounds for the failover:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;It identifies existing master&lt;/li&gt; &lt;li&gt;It refactors the topology to have representatives of all three data centers under the master. Different DCs have different network latencies and are expected to react in different timing to master’s crash.&lt;/li&gt; &lt;li&gt;It chooses a crash method. We choose from shooting the master (&lt;code class=&quot;highlighter-rouge&quot;&gt;kill -9&lt;/code&gt;) or network partitioning it: &lt;code class=&quot;highlighter-rouge&quot;&gt;iptables -j REJECT&lt;/code&gt; (nice-ish) or &lt;code class=&quot;highlighter-rouge&quot;&gt;iptables -j DROP&lt;/code&gt; (unresponsive).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The script proceeds to crash the master by chosen method, and waits for &lt;code class=&quot;highlighter-rouge&quot;&gt;orchestrator&lt;/code&gt; to reliably detect the crash and to perform failover. While we expect detection and promotion to both complete within &lt;code class=&quot;highlighter-rouge&quot;&gt;30&lt;/code&gt; seconds, the script relaxes this expectation a bit, and sleeps for a designated time before looking into failover results. It will then:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;Check that a new (different) master is in place&lt;/li&gt; &lt;li&gt;There is a good number of replicas in the cluster&lt;/li&gt; &lt;li&gt;The master is writable&lt;/li&gt; &lt;li&gt;Writes to the master are visible on the replicas&lt;/li&gt; &lt;li&gt;Internal service discovery entries are updated (identity of new master is as expected; old master removed)&lt;/li&gt; &lt;li&gt;Other internal checks&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These tests confirm that the failover was successful, not only MySQL-wise but also on our larger infrastructure scope. A VIP has been assumed; specific services have been started; information got to where it was supposed to go.&lt;/p&gt;&lt;p&gt;The script further proceeds to restore the failed server:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;Restoring it from backup, thereby implicitly testing our backup/restore procedure&lt;/li&gt; &lt;li&gt;Verifying server configuration is as expected (the server no longer believes it’s the master)&lt;/li&gt; &lt;li&gt;Returning it to the replication cluster, expecting to find data written on the master&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Consider the following visualization of a scheduled failover test: from having a well-running cluster, to seeing problems on some replicas, to diagnosing the master (&lt;code class=&quot;highlighter-rouge&quot;&gt;7136&lt;/code&gt;) is dead, to choosing a server to promote (&lt;code class=&quot;highlighter-rouge&quot;&gt;a79d&lt;/code&gt;), refactoring the topology below that server, to promoting it (failover successful), to restoring the dead master and placing it back into the cluster.&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;automated master failover&quot; src=&quot;/images/mysql-infrastructre-testing-automation/orchestrator-failover-demo-2000.gif&quot; /&gt;&lt;/p&gt;&lt;h4 id=&quot;what-would-a-test-failure-look-like&quot;&gt;What would a test failure look like?&lt;/h4&gt;&lt;p&gt;Our testing script uses a stop-the-world approach. A single failure in any of the failover components fails the entire test, disabling any future automated tests until a human resolves the matter. We get alerted and proceed to check the status and logs.&lt;/p&gt;&lt;p&gt;The script would fail on an unacceptable detection or failover time; on backup/restore issues; on losing too many servers; on unexpected configuration following the failover; etc.&lt;/p&gt;&lt;p&gt;We need to be certain &lt;code class=&quot;highlighter-rouge&quot;&gt;orchestrator&lt;/code&gt; connects the servers correctly. This is where the contending write load comes useful: if set up incorrectly, replication is easily susceptible to break. We would get &lt;code class=&quot;highlighter-rouge&quot;&gt;DUPLICATE KEY&lt;/code&gt; or other errors to suggest something went wrong.&lt;/p&gt;&lt;p&gt;This is particularly important as we make improvements and introduce new behavior to &lt;code class=&quot;highlighter-rouge&quot;&gt;orchestrator&lt;/code&gt;, and allows us to test such changes in a safe environment.&lt;/p&gt;&lt;h4 id=&quot;coming-up-chaos-testing&quot;&gt;Coming up: chaos testing&lt;/h4&gt;&lt;p&gt;The testing procedure illustrated above will catch (and has caught) problems on many parts of our infrastructure. Is it enough?&lt;/p&gt;&lt;p&gt;In a production environment there’s always something else. Something about the particular test method that won’t apply to our production clusters. They don’t share the same traffic and traffic manipulation, nor the exact same set of servers. The types of failure can vary.&lt;/p&gt;&lt;p&gt;We are designing chaos testing for our production clusters. Chaos testing would literally destroy pieces in our production, but on expected schedule and under sufficiently controlled manner. Chaos testing introduces a higher level of trust in the recovery mechanism and affects (thus tests) larger parts of our infrastructure and application.&lt;/p&gt;&lt;p&gt;This is delicate work: while we acknowledge the need for chaos testing, we also wish to avoid unnecessary impact to our service. Different tests will differ in risk level and impact, and we will work to ensure availability of our service.&lt;/p&gt;&lt;h2 id=&quot;schema-migrations&quot;&gt;Schema migrations&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://githubengineering.com/gh-ost-github-s-online-migration-tool-for-mysql/&quot;&gt;We use gh-ost&lt;/a&gt; to run live schema migrations. &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; is stable, but also under active developments, with major new features being added or planned.&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; migrates tables by copying data onto a &lt;em&gt;ghost&lt;/em&gt; table, applying ongoing changes intercepted by the binary logs onto the &lt;em&gt;ghost&lt;/em&gt; table, even as the original table is being written to. It then swaps the &lt;em&gt;ghost&lt;/em&gt; table in place of the original table. At migration completion GitHub proceeds to work with a table generated and populated by &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;At this time almost all of GitHub’s MySQL data has been recreated by &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt;, and most of it multiple times. We must have high trust in &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; to let it tamper with our data over and over again, even in face of active development. Here’s how we gain this trust.&lt;/p&gt;&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; provides a testing-in-production capability. It supports running a migration on a replica, in much the same way as it would run on the master: &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; would connect to the replica and treat it as if it were the master. It would parse its binary logs the same way it would for a real master migration. However it would copy rows and apply binlog events to the replica, and avoid making writes onto the master.&lt;/p&gt;&lt;p&gt;We run &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt;-dedicated replicas in production. These replicas do not serve production traffic. Each such replica retrieves the current list of production tables and iterates them in random order. One by one it picks a table and performs a replica-migration on that table. The migration doesn’t actually modify table structure, but instead runs a trivial &lt;code class=&quot;highlighter-rouge&quot;&gt;ENGINE=InnoDB&lt;/code&gt;. The test runs the migration even as the table is being used in production, thus copying real production data and applying true production traffic off the binary logs.&lt;/p&gt;&lt;p&gt;These migrations can be audited. Here’s how we can inspect status of running tests from chat:&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars0.githubusercontent.com/ggunson?v=3&amp;amp;s=22&quot; alt=&quot;ggunson&quot; srcset=&quot;https://avatars0.githubusercontent.com/ggunson?v=3&amp;amp;s=22 1x, https://avatars0.githubusercontent.com/ggunson?v=3&amp;amp;s=44 2x, https://avatars0.githubusercontent.com/ggunson?v=3&amp;amp;s=66 3x, https://avatars0.githubusercontent.com/ggunson?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;ggunson&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .migration test-status &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt;&lt;pre&gt;# Migrating `prod`.`pull_requests`; Ghost table is `prod`.`_pull_requests_gho`# Migrating ghost-db-mysql-0007:3306; inspecting ghost-db-mysql-0007:3306; executing on ghost-db-mysql-0007# Migration started at Mon Jan 30 02:13:39 -0800 2017# chunk-size: 2500; max-lag-millis: 1500ms; max-load: Threads_running=30; critical-load: Threads_running=1000; nice-ratio: 0.000000# throttle-additional-flag-file: /tmp/gh-ost.throttle# panic-flag-file: /tmp/ghost-test-panic.flag# Serving on unix socket: /tmp/gh-ost.test.sockCopy: 57992500/86684838 66.9%; Applied: 57708; Backlog: 1/100; Time: 3h28m38s(total), 3h28m36s(copy); streamer: mysql-bin.000576:142993938; State: migrating; ETA: 1h43m12s&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;p&gt;When a test migration completes copying of table data it stops replication and performs the cut-over, replacing the original table with the &lt;em&gt;ghost&lt;/em&gt; table, and then swaps back. We’re not interested in actually replacing the data. Instead we are left with both the original table and the &lt;em&gt;ghost&lt;/em&gt; table, which should both be identical. We verify that by checksumming the entire table data for both tables.&lt;/p&gt;&lt;p&gt;A test can complete with:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;success&lt;/em&gt;: All went well and checksum is identical. We expect to see this.&lt;/li&gt; &lt;li&gt;&lt;em&gt;failure&lt;/em&gt;: Execution problem. This can occasionally happen due to the migration process being killed, a replication issue etc., and is typically unrelated to &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; itself.&lt;/li&gt; &lt;li&gt;&lt;em&gt;checksum failure&lt;/em&gt;: table data inconsistency. For a tested branch, this call for fixes. For an ongoing &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt; branch test, this would imply immediate blocking of production migrations. We don’t get the latter.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Test results are audited, sent to robot chatrooms, sent as events to our metrics systems. Each vertical line in the following graph represents a successful migration test:&lt;/p&gt;&lt;p&gt;&lt;img alt=&quot;automated master failover&quot; src=&quot;/images/mysql-infrastructre-testing-automation/gh-ost-tests-results-as-events.png&quot; /&gt;&lt;/p&gt;&lt;p&gt;These tests run continuously. We are notified by alerts in case of failures. And of course we can always visit the robots chatroom to know what’s going on.&lt;/p&gt;&lt;h4 id=&quot;testing-new-versions&quot;&gt;Testing new versions&lt;/h4&gt;&lt;p&gt;We continuously improve &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt;. Our development flow is based on &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; branches, which we then offer to merge via &lt;a href=&quot;https://github.com/github/gh-ost/pulls&quot;&gt;pull requests&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;A submitted &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; pull request goes through Continuous Integration (CI) which runs basic compilation and unit tests. Once past this, the PR is technically eligible for merging, but even more interestingly it is &lt;a href=&quot;https://githubengineering.com/deploying-branches-to-github-com/&quot;&gt;eligible for deployment via Heaven&lt;/a&gt;. Being the sensitive component in our infrastructure that it is, we take care to deploy &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; branches for intensive testing before merging into &lt;code class=&quot;highlighter-rouge&quot;&gt;master&lt;/code&gt;.&lt;/p&gt;&lt;div class=&quot;chat&quot; style=&quot;margin: 30px 0;&quot;&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=22&quot; alt=&quot;shlomi-noach&quot; srcset=&quot;https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=22 1x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=44 2x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=66 3x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;shlomi-noach&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .deploy gh-ost/fix-reappearing-throttled-reasons to prod/ghost-db-mysql-0007 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; @shlomi-noach is deploying gh-ost/fix-reappearing-throttled-reasons (baee4f6) to production (ghost-db-mysql-0007). &lt;br /&gt; @shlomi-noach's production deployment of gh-ost/fix-reappearing-throttled-reasons (baee4f6) is done! (2s) &lt;br /&gt; @shlomi-noach, make sure you watch for exceptions in haystack &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars0.githubusercontent.com/jonahberquist?v=3&amp;amp;s=22&quot; alt=&quot;jonahberquist&quot; srcset=&quot;https://avatars0.githubusercontent.com/jonahberquist?v=3&amp;amp;s=22 1x, https://avatars0.githubusercontent.com/jonahberquist?v=3&amp;amp;s=44 2x, https://avatars0.githubusercontent.com/jonahberquist?v=3&amp;amp;s=66 3x, https://avatars0.githubusercontent.com/jonahberquist?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;jonahberquist&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .deploy gh-ost/interactive-command-question to prod/ghost-db-mysql-0012 &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; @jonahberquist is deploying gh-ost/interactive-command-question (be1ab17) to production (ghost-db-mysql-0012). &lt;br /&gt; @jonahberquist's production deployment of gh-ost/interactive-command-question (be1ab17) is done! (2s) &lt;br /&gt; @jonahberquist, make sure you watch for exceptions in haystack &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message self&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=22&quot; alt=&quot;shlomi-noach&quot; srcset=&quot;https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=22 1x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=44 2x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=66 3x, https://avatars3.githubusercontent.com/shlomi-noach?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;shlomi-noach&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; .wcid gh-ost &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;message robot&quot;&gt; &lt;img class=&quot;avatar avatar-small&quot; src=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22&quot; alt=&quot;hubot&quot; srcset=&quot;https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=22 1x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=44 2x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=66 3x, https://avatars1.githubusercontent.com/hubot?v=3&amp;amp;s=88 4x&quot; width=&quot;22&quot; height=&quot;22&quot; data-proofer-ignore=&quot;true&quot; /&gt; &lt;b class=&quot;author&quot;&gt;Hubot&lt;/b&gt; &lt;div class=&quot;entry&quot;&gt; shlomi-noach testing fix-reappearing-throttled-reasons 41 seconds ago: ghost-db-mysql-0007 &lt;br /&gt; jonahberquist testing interactive-command-question 7 seconds ago: ghost-db-mysql-0012 &lt;br /&gt; &lt;br /&gt; Nobody is in the queue. &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;p&gt;Some PRs are small and do not affect the data itself. Changes to status messages, interactive commands etc. are of lesser impact to the &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-ost&lt;/code&gt; app. Others pose significant changes to the migration logic and operation. We would tests these rigorously, running through our production tables fleet until satisfied these changes do not pose data corruption threat.&lt;/p&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;&lt;p&gt;Throughout testing we build trust in our systems. By automating these tests, in production, we get repetitive confirmation that everything is working as expected. As we continue to develop our infrastructure we also follow up by adapting tests to cover the newest changes.&lt;/p&gt;&lt;p&gt;Production always surprises with scenarios not covered by tests. The more we test on production environment, the more input we get on our app’s expectations and our infrastructure’s capabilities.&lt;/p&gt;</content><author><name>{&quot;username&quot;=&gt;&quot;tomkrouper&quot;, &quot;fullname&quot;=&gt;&quot;Tom Krouper&quot;, &quot;twitter&quot;=&gt;&quot;CaptainEyesight&quot;, &quot;role&quot;=&gt;&quot;Senior Infrastructure Engineer&quot;, &quot;links&quot;=&gt;[{&quot;name&quot;=&gt;&quot;GitHub Profile&quot;, &quot;url&quot;=&gt;&quot;https://github.com/tomkrouper&quot;}, {&quot;name&quot;=&gt;&quot;Twitter Profile&quot;, &quot;url&quot;=&gt;&quot;https://twitter.com/captaineyesight&quot;}]}</name></author><summary type="html">Our MySQL infrastructure is a critical component to GitHub. MySQL serves GitHub.com, GitHub’s API, authentication and more. Every git request touches MySQL in some way. We are tasked with keeping the data available, and maintaining its integrity. Even while our MySQL clusters serve traffic, we need to be able to perform tasks such as heavy duty cleanups, ad-hoc updates, online schema migrations, cluster topology refactoring, pooling and load balancing and more. We have the infrastructure to automate away such operations; in this post we share a few examples of how we build trust in our infrastructure through continuous testing. It is essentially how we sleep well at night.</summary></entry><entry><title type="html">DNS Infrastructure at GitHub</title><link href="https://githubengineering.com/dns-infrastructure-at-github/" rel="alternate" type="text/html" title="DNS Infrastructure at GitHub" /><published>2017-05-31T00:00:00+00:00</published><updated>2017-05-31T00:00:00+00:00</updated><id>https://githubengineering.com/dns-infrastructure-at-github</id><content type="html" xml:base="https://githubengineering.com/dns-infrastructure-at-github/">&lt;p&gt;At GitHub we recently revamped how we do DNS from the ground up. This included both how we &lt;a href=&quot;https://githubengineering.com/enabling-split-authority-dns-with-octodns/&quot;&gt;interact with external DNS providers&lt;/a&gt; and how we serve records internally to our hosts. To do this, we had to design and build a new DNS infrastructure that could scale with GitHub’s growth and across many data centers.&lt;/p&gt;&lt;p&gt;Previously GitHub’s DNS infrastructure was fairly simple and straightforward. It included a local, forwarding only DNS cache on every server and a pair of hosts that acted as both caches and authorities used by all these hosts. These hosts were available both on the internal network as well as public internet. We configured zone stubs in the caching daemon to direct queries locally rather than recurse on the internet. We also had NS records set up at our DNS providers that pointed specific internal zones to the public IPs of this pair of hosts for queries external to our network.&lt;/p&gt;&lt;p&gt;This configuration worked for many years but was not without its downsides. Many applications are highly sensitive to resolving DNS queries and any performance or availability issues we ran into would cause queuing and degraded performance at best and customer impacting outages at worst. Configuration and code changes can cause large unexpected changes in query rates. As such scaling beyond these two hosts became an issue. Due to the network configuration of these hosts we would just need to keep adding IPs and hosts which has its own problems. While attempting to fire fight and remediate these issues, the old system made it difficult to identify causes due to a lack of metrics and visibility. In many cases we resorted to &lt;code class=&quot;highlighter-rouge&quot;&gt;tcpdump&lt;/code&gt; to identify traffic and queries in question. Another issue was running on public DNS servers we run the risk of leaking internal network information. As a result we decided to build something better and began to identify our requirements for the new system.&lt;/p&gt;&lt;p&gt;We set out to design a new DNS infrastructure that would improve the aforementioned operational issues including scaling and visibility, as well as introducing some additional requirements. We wanted to continue to run our public DNS zones via external DNS providers so whatever system we build needed to be vendor agnostic. Additionally, we wanted this system to be capable of serving both our internal and external zones, meaning internal zones were only available on our internal network unless specifically configured otherwise and external zones are resolvable without leaving our internal network. We wanted the new DNS architecture to allow both a &lt;a href=&quot;https://githubengineering.com/enabling-split-authority-dns-with-octodns/&quot;&gt;deploy-based workflow for making changes&lt;/a&gt; as well as API access to our records for automated changes via our inventory and provisioning systems. The new system could not have any external dependencies; too much relies on DNS functioning for it to get caught in a cascading failure. This includes connectivity to other data centers and DNS services that may reside there. Our old system mixed the use of caches and authorities on the same host; we wanted to move to a tiered design with isolated roles. Lastly, we wanted a system that could support many data center environments whether it be EC2 or bare metal.&lt;/p&gt;&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://dl.dropboxusercontent.com/s/tx6s7zg896x6kav/2017-05-09%20at%2012.14%20PM.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;&lt;p&gt;To build this system we identified three classes of hosts: caches, edges, and authorities. Caches serve as recursive resolvers and DNS “routers” caching responses from the edge tier. The edge tier, running a DNS authority daemon, responds to queries from the caching tier for zones it is configured to zone transfer from the authority tier. The authority tier serve as hidden DNS masters as our canonical source for DNS data, servicing zone transfers from the edge hosts as well as providing an HTTP API for creating, modifying or deleting records.&lt;/p&gt;&lt;p&gt;In our new configuration, caches live in each data center meaning application hosts don’t need to traverse a data center boundary to retrieve a record. The caches are configured to map zones to the edge hosts within their region in order to route our internal zones to our own hosts. Any zone that is not explicitly configured will recurse on the internet to resolve an answer.&lt;/p&gt;&lt;p&gt;The edge hosts are regional hosts, living in our network edge PoPs (Point of Presence). Our PoPs have one or more data centers that rely on them for external connectivity, without the PoP the data center can’t get to the internet and the internet can’t get to them. The edges perform zone transfers with all authorities regardless of what region or location they exist in and store those zones locally on their disk.&lt;/p&gt;&lt;p&gt;Our authorities are also regional hosts, only containing zones applicable to the region it is contained in. Our inventory and provisioning systems determine which regional authority a zone lives in and will create and delete records via an HTTP API as servers come and go. OctoDNS maps zones to regional authorities and uses the same API to create static records and to ensure dynamic sources are in sync. We have an additional separate authority for external domains, such as github.com, to allow us to query our external domains during a disruption to connectivity. All records are stored in MySQL.&lt;/p&gt;&lt;h3 id=&quot;operability&quot;&gt;Operability&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://dl.dropboxusercontent.com/s/jw8bjx8oattik7w/2017-05-09%20at%2011.52%20AM.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;&lt;p&gt;One huge benefit of moving to a more modern DNS infrastructure is observability. Our old DNS system had little to no metrics and limited logging. A large factor in deciding which DNS servers to use was the breadth and depth of metrics they produce. We finalized on &lt;a href=&quot;https://unbound.net&quot;&gt;Unbound&lt;/a&gt; for the caches, &lt;a href=&quot;https://www.nlnetlabs.nl/projects/nsd/&quot;&gt;NSD&lt;/a&gt; for the edge hosts and &lt;a href=&quot;https://powerdns.com/&quot;&gt;PowerDNS&lt;/a&gt; for the authorities, all of which have been proven in DNS infrastructures much larger than at GitHub.&lt;/p&gt;&lt;p&gt;When running in our bare metal data centers, caches are accessed via a private &lt;a href=&quot;https://en.wikipedia.org/wiki/Anycast&quot;&gt;anycast&lt;/a&gt; IP resulting in it reaching the nearest available cache host. The caches have been deployed in a rack aware manner that provides some level of balanced load between them and isolation against some power and network failure modes. When a cache host fails, servers that would normally use it for lookups will now automatically be routed to the next closest cache, keeping latency low as well as providing tolerance to some failure modes. Anycast allows us to scale the number of caches behind a single IP address unlike our previous configuration, giving us the ability to run as many caching hosts as DNS demand requires.&lt;/p&gt;&lt;p&gt;Edge hosts perform zone transfers with the authority tier, regardless of region or location. Our zones are not large enough that keeping a copy of all of them in every region is a problem. This means for every zone, all caches will have access to a local edge server with a local copy of all zones even when a region is offline or upstream providers are having connectivity issues. This change alone has proven to be quite resilient in the face of connectivity issues and has helped keep GitHub available during failures that not long ago would have caused customer facing outages.&lt;/p&gt;&lt;p&gt;These zone transfers include both our internal and external zones from their respective authorities. As you might guess zones like github.com are external and zones like github.net are generally internal. The difference between them is only the types of use and data stored in them. Knowing which zones are internal and external gives us some flexibility in our configuration.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dig +short github.com192.30.253.112192.30.253.113&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Public zones are &lt;a href=&quot;https://githubengineering.com/enabling-split-authority-dns-with-octodns/&quot;&gt;sync’d&lt;/a&gt; to external DNS providers and are records GitHub users use everyday. Addtionally, public zones are completely resolvable within our network without needing to communicate with our external providers. This means any service that needs to look up &lt;code class=&quot;highlighter-rouge&quot;&gt;api.github.com&lt;/code&gt; can do so without needing to rely on external network connectivity. We also use the stub-first configuration option of Unbound which gives a lookup a second chance if our internal DNS service is down for some reason by looking it up externally when it fails.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dig +short time.github.net10.127.6.10&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Most of the &lt;code class=&quot;highlighter-rouge&quot;&gt;github.net&lt;/code&gt; zone is completely private, inaccessible from the internet and only contains &lt;a href=&quot;http://www.faqs.org/rfcs/rfc1918.html&quot;&gt;RFC 1918&lt;/a&gt; IP addresses. Private zones are split up per region and site. Each region and/or site has a set of sub-zones applicable to that location, sub-zones for management network, service discovery, specific service records and yet to be provisioned hosts that are in our inventory. Private zones also include reverse lookup zones for PTRs.&lt;/p&gt;&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;&lt;p&gt;Replacing an old system with a new one that is ready to serve millions of customers is never easy. Using a pragmatic, requirements based approach to designing and implementing our new DNS system resulted in a DNS infrastructure that was able to hit the ground running and will hopefully grow with GitHub into the future.&lt;/p&gt;&lt;p&gt;Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us. &lt;a href=&quot;https://boards.greenhouse.io/github/jobs/669805#.WPVqJlPyvUI&quot;&gt;Apply Here&lt;/a&gt;&lt;/p&gt;</content><author><name>Joe Williams</name></author><summary type="html">At GitHub we recently revamped how we do DNS from the ground up. This included both how we interact with external DNS providers and how we serve records internally to our hosts. To do this, we had to design and build a new DNS infrastructure that could scale with GitHub’s growth and across many data centers.</summary></entry><entry><title type="html">Integrating Git in Atom</title><link href="https://githubengineering.com/integrating-git-in-atom/" rel="alternate" type="text/html" title="Integrating Git in Atom" /><published>2017-05-16T00:00:00+00:00</published><updated>2017-05-16T00:00:00+00:00</updated><id>https://githubengineering.com/integrating-git-in-atom</id><content type="html" xml:base="https://githubengineering.com/integrating-git-in-atom/">&lt;p&gt;The Atom team has been working to bring the power of Git and GitHub as close toyour cursor as possible. With &lt;a href=&quot;https://blog.atom.io/2017/05/16/git-and-github-integration-comes-to-atom.html&quot;&gt;today’s release of the GitHub package forAtom&lt;/a&gt;, youcan now perform common Git operations without leaving the editor: stage changes,make commits, create and switch branches, resolve merge conflicts, and more.&lt;/p&gt;&lt;p&gt;In this post, we’ll look at the evolution of how the &lt;a href=&quot;https://github.com/atom/github&quot;&gt;Atom GitHub package&lt;/a&gt;interacts with the &lt;code class=&quot;highlighter-rouge&quot;&gt;.git&lt;/code&gt; folder in your project.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.atom.io/&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/github-for-atom.png&quot; alt=&quot;GitHub for Atom&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;interacting-with-git&quot;&gt;Interacting with Git&lt;/h2&gt;&lt;p&gt;GitHub is a core contributor to a library called &lt;a href=&quot;https://libgit2.github.com/&quot;&gt;libgit2&lt;/a&gt;, which is a reentrant C implementation of Git’s core methods and is used to power the backend of GitHub.com via Ruby bindings. Our initial approach to the development of this new Atom package used &lt;a href=&quot;http://www.nodegit.org/&quot;&gt;Nodegit&lt;/a&gt;, a Node module that provides native bindings to libgit2.&lt;/p&gt;&lt;p&gt;Months into development we started to question whether this was the optimal approach for our Atom integration. libgit2 is a powerful library that implements the core data structures and algorithms of the Git version control system, but it intentionally implements only a subset of the system. While it is &lt;a href=&quot;https://githubengineering.com/move-fast/#merges-in-libgit2&quot;&gt;very effective&lt;/a&gt; as a technology that powers the backend of GitHub.com, our use case is sufficiently different and more akin to the Git command-line experience.&lt;/p&gt;&lt;p&gt;Compare what we had to do with Nodegit/libgit2 versus shelling out:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Nodegit/libgit2&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt; &lt;li&gt;Read the current index file&lt;/li&gt; &lt;li&gt;Update the files that have changed&lt;/li&gt; &lt;li&gt;Create a tree with this state&lt;/li&gt; &lt;li&gt;Write the updated index back to disk&lt;/li&gt; &lt;li&gt;Manually run pre-commit hooks&lt;/li&gt; &lt;li&gt;Create the new commit with the tree&lt;/li&gt; &lt;li&gt;Manually sign the commit if necessary&lt;/li&gt; &lt;li&gt;Update the currently active branch to point to the commit&lt;/li&gt; &lt;li&gt;Manually run post-commit hook&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Shelling out&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt; &lt;li&gt;Run &lt;code class=&quot;highlighter-rouge&quot;&gt;git commit&lt;/code&gt;, the command-line tool made for our exact use case&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Shelling out to Git simplifies development, gives us access to the full set of commands, options, and formatting that Git core provides, and enables us to use all of the latest Git features without having to reimplement custom logic or wait for support in libgit2. For these reasons and more, we made the switch.&lt;/p&gt;&lt;h2 id=&quot;reboot-to-shelling-out&quot;&gt;Reboot to shelling out&lt;/h2&gt;&lt;p&gt;We bundled a minimal version of Git for Mac, Windows, and Linux into a package called &lt;a href=&quot;https://github.com/desktop/dugite-native&quot;&gt;dugite-native&lt;/a&gt; and created a lightweight library called &lt;a href=&quot;https://github.com/desktop/dugite&quot;&gt;dugite&lt;/a&gt; for making Node &lt;code class=&quot;highlighter-rouge&quot;&gt;execFile&lt;/code&gt; calls. Bundling Git makes package installation easier for the user and gives us full control over the Git API we are interacting with.&lt;/p&gt;&lt;p&gt;As much as possible, we keep your Git data in Atom in sync with the actual state of your local repo to allow for maximal flexibility. You can partially stage a file in Atom, switch to the command line and find the state of your repo exactly as you’d expect. Additionally, any changes you make outside of Atom will be detected by a file watcher and the Git data in your editor will be refreshed automatically.&lt;/p&gt;&lt;p&gt;Overall, the transition from Nodegit to shelling out went pretty well. However, there were noticeable performance tradeoffs and overhead costs associated with spawning a new process every time we asked for Git data.&lt;/p&gt;&lt;h2 id=&quot;performance-concerns-and-optimizations&quot;&gt;Performance concerns and optimizations&lt;/h2&gt;&lt;p&gt;Recent Atom releases &lt;a href=&quot;http://blog.atom.io/2017/04/18/improving-startup-time.html&quot;&gt;have&lt;/a&gt; &lt;a href=&quot;https://github.com/atom/text-buffer/pull/185&quot;&gt;delivered&lt;/a&gt; &lt;a href=&quot;http://blog.atom.io/2017/01/10/atom-1-13.html#atom-benchmarks&quot;&gt;numerous&lt;/a&gt; &lt;a href=&quot;http://blog.atom.io/2017/01/10/atom-1-13.html#performance-boosters&quot;&gt;performance&lt;/a&gt; &lt;a href=&quot;http://blog.atom.io/2017/04/12/atom-1-16.html#farewell-to-jquery&quot;&gt;improvements&lt;/a&gt;, and we wanted this new package to demonstrate our continued focus on responsiveness. After core functionality was in place, we introduced a series of optimizations. To inform and measure progress on this front, we created a &lt;a href=&quot;https://github.com/atom/github/pull/508&quot;&gt;custom waterfall view&lt;/a&gt; to visualize the time spent shelling out to Git; the red section shows the time an operation spent waiting in the queue for its turn to run, while the yellow and green represent the time the operation took to actually execute.&lt;/p&gt;&lt;p&gt;Here’s what it looked like before and after we &lt;a href=&quot;https://github.com/atom/github/pull/518&quot;&gt;parallelized read operations&lt;/a&gt; based on the number of cores on a user’s computer:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;/images/making-git-atomic/parallel-before-after.png&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/parallel-before-after.png&quot; alt=&quot;Parallelizing read operations: before 53ms, after 29ms&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;We also noticed that for larger repos we would get file-watching update events in several batches, each causing a model update to be scheduled. A merge with conflicts in &lt;code class=&quot;highlighter-rouge&quot;&gt;github/github&lt;/code&gt;, the GitHub.com codebase, would queue up 12 updates. To address this we redesigned our &lt;a href=&quot;https://github.com/atom/github/pull/541/files#diff-b82da88b19ff2e1ef893b0fa726769b8&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ModelObserver&lt;/code&gt;&lt;/a&gt; to never schedule more than a single pending fetch if new update requests come in while a fetch is in progress, &lt;a href=&quot;https://github.com/atom/github/pull/541&quot;&gt;preventing ModelObserver update backlogs&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;/images/making-git-atomic/model-observer-before-after.png&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/model-observer-before-after.png&quot; alt=&quot;Prevent ModelObserver update backlogs: before 91 events taking 6s, after 37 events taking 2.5s&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Aggressive &lt;a href=&quot;https://github.com/atom/github/pull/700&quot;&gt;caching&lt;/a&gt; and selectively invalidating cached repository state reduced the number of times we shell out to Git so that we avoid the performance penalty of launching a new process:&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;/images/making-git-atomic/caching-before-after.png&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/caching-before-after.png&quot; alt=&quot;Caching — before 17 events taking 205ms, after 5 events taking 57ms&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Even though we spawn subprocesses asynchronously, there is still a small synchronous overhead to shelling out to Git. Normally, this is no more than a couple milliseconds. On rare occasions, however, the application would get into a strange state, and this time would begin to grow; this overhead is represented in the waterfall views above by the yellow sections. The additional time spent in synchronous code would block the UI thread long enough to degrade the user experience. The issue would persist until the Atom window was refreshed.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;/images/making-git-atomic/spawn-timeline-annotated.png&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/spawn-timeline-annotated.png&quot; alt=&quot;Node.js spawn call taking 50ms synchronous time each usage&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;After investigating the root cause of this issue, we realized that a proper fix for it would have involved changing Node or libuv. Since our launch date was looming on the horizon, we needed a more immediate solution and made the decision to work around the problem by making Git calls in a separate process. This would keep the main thread free and prevent locking the UI when this issue arises.&lt;/p&gt;&lt;h2 id=&quot;shelling-out-in-a-dedicated-side-process&quot;&gt;Shelling out in a dedicated side process&lt;/h2&gt;&lt;p&gt;Our first approach used forked Node processes, but benchmarking revealed that IPC time grows quadratically relative to message size, which could become an issue when reading large diffs from stdout. This issue seems to be &lt;a href=&quot;https://github.com/nodejs/node/pull/10557&quot;&gt;fixed&lt;/a&gt; in future versions of Node, but again, time was of the essence and we couldn’t afford to wait. Thankfully, IPC times using Electron renderer processes were much more reasonable, so our short term solution involved using a &lt;a href=&quot;https://github.com/atom/github/pull/688&quot;&gt;dedicated renderer process&lt;/a&gt; to run Git commands.&lt;/p&gt;&lt;p&gt;We introduced a &lt;a href=&quot;https://github.com/atom/github/blob/v0.0.6/lib/worker-manager.js#L11&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WorkerManager&lt;/code&gt;&lt;/a&gt; which creates a &lt;a href=&quot;https://github.com/atom/github/blob/v0.0.6/lib/worker-manager.js#L109&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Worker&lt;/code&gt;&lt;/a&gt; that wraps a &lt;a href=&quot;https://github.com/atom/github/blob/v0.0.6/lib/worker-manager.js#L261&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RendererProcess&lt;/code&gt;&lt;/a&gt; which shells out to Git and sends results back over IPC. If the renderer process is not yet ready, we fall back to shelling out in process. We track a running average of the time it takes to make a spawn call and if this exceeds a specified threshold, the &lt;code class=&quot;highlighter-rouge&quot;&gt;WorkerManager&lt;/code&gt; creates a new &lt;code class=&quot;highlighter-rouge&quot;&gt;Worker&lt;/code&gt; and routes all new Git data requests to it. With this approach, if the long spawn call issue manifests, users will experience no freezing due to a blocked main thread. At worst, they may experience slower UI updates, but once a new renderer process is up the spawn times should drop back down and normal responsiveness should be restored.&lt;/p&gt;&lt;p&gt;As with most decisions, there are tradeoffs. Here we prevent indefinite locking of the UI, but there is now extra time spent in IPC and overhead costs associated with creating new Electron renderer processes. In the timeline below, the pink represents the IPC time associated with each Git command.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;/images/making-git-atomic/waterfall-with-ipc.png&quot;&gt;&lt;img src=&quot;/images/making-git-atomic/waterfall-with-ipc.png&quot; alt=&quot;IPC adds a couple dozen seconds of overhead to Git calls&quot; /&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Once we &lt;a href=&quot;https://github.com/atom/atom/pull/12696&quot;&gt;upgrade Atom to Electron v1.6.x&lt;/a&gt; in the next release cycle, we’ll be able to re-implement this system using Chromium Web Workers with Node integration. Using the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer&quot;&gt;SharedArrayBuffer&lt;/a&gt; object, we can read shared memory and bypass IPC, cutting down overall operation time. And using native Web Workers rather than Electron Renderer Processes will reduce the overhead associated with these side processes and save on computing resources for shelling out to Git.&lt;/p&gt;&lt;h2 id=&quot;continuing-the-vision-for-atom&quot;&gt;Continuing the vision for Atom&lt;/h2&gt;&lt;p&gt;In addition to more performance improvements, you can look forward to more Git features, UI/UX improvements, and more comprehensive and in-depth GitHub integration.&lt;/p&gt;&lt;p&gt;As developers, much of our work is powered by a few key tools that enable us to write software and collaborate. We spend most of our days in our editors, periodically pausing to take version control snapshots of our code, and soliciting input and feedback from our colleagues. It’s every developer’s dream to be able to do all of these things with minimal friction and maximal ease. With these new integrations the Atom team is working to make those dreams more of a reality.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to help the Atom team make developers’ lives easier?&lt;/strong&gt; We’d love for you to join us. Keep an eye out for a &lt;a href=&quot;https://github.com/about/careers#positions&quot;&gt;job posting&lt;/a&gt; coming soon!&lt;/p&gt;</content><author><name>Katrina Uychaco</name></author><summary type="html">The Atom team has been working to bring the power of Git and GitHub as close to your cursor as possible. With today’s release of the GitHub package for Atom, you can now perform common Git operations without leaving the editor: stage changes, make commits, create and switch branches, resolve merge conflicts, and more.</summary></entry><entry><title type="html">How Four Native Developers Wrote An Electron App</title><link href="https://githubengineering.com/how-four-native-developers-wrote-an-electron-app/" rel="alternate" type="text/html" title="How Four Native Developers Wrote An Electron App" /><published>2017-05-16T00:00:00+00:00</published><updated>2017-05-16T00:00:00+00:00</updated><id>https://githubengineering.com/how-four-native-developers-wrote-an-electron-app</id><content type="html" xml:base="https://githubengineering.com/how-four-native-developers-wrote-an-electron-app/">&lt;p&gt;Today we released the new &lt;a href=&quot;https://desktop.github.com/&quot;&gt;GitHub Desktop Beta&lt;/a&gt;, rewritten on &lt;a href=&quot;http://electron.atom.io/&quot;&gt;Electron&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Electron is a well-known on-ramp for web developers to build desktop apps using familiar web technologies: HTML, CSS, and JavaScript. Our situation was different. Everyone on the GitHub Desktop team is a native developer by trade—three from the .NET world and one from Cocoa. We knew how to make native apps, so how and why did we end up here?&lt;/p&gt;&lt;h2 id=&quot;why-rewrite&quot;&gt;Why Rewrite?&lt;/h2&gt;&lt;p&gt;First, the elephant in the room: why? Rewrites are rarely a good idea. Why did we decide to walk away from two codebases and rewrite?&lt;/p&gt;&lt;p&gt;From the start, GitHub Desktop for macOS and Windows were two distinct products, each with their own team. We worked in two separate tech stacks using two different skill sets. To maintain parity across the codebases, we had to implement and design the same features twice. If we ever wanted to add Linux support, we’d have to do it all a third time. All this meant we had twice the work, twice the bugs, and far less time to build new features.&lt;/p&gt;&lt;p&gt;As it turns out, building native apps for multiple platforms doesn’t scale.&lt;/p&gt;&lt;h2 id=&quot;why-electron&quot;&gt;Why Electron?&lt;/h2&gt;&lt;p&gt;This isn’t a new or unique problem. Over the years we explored various ways for evolving the existing applications towards a shared codebase, including &lt;a href=&quot;http://electron.atom.io/&quot;&gt;Electron&lt;/a&gt;, &lt;a href=&quot;https://www.xamarin.com&quot;&gt;Xamarin&lt;/a&gt;, a shared C++, or in our wildest dreams, Haskell.&lt;/p&gt;&lt;p&gt;We’d already &lt;a href=&quot;https://githubengineering.com/cross-platform-ui-in-github-desktop/&quot;&gt;experimented with web technologies&lt;/a&gt; to share work. The gravitational pull of the web was too strong to resist.&lt;/p&gt;&lt;p&gt;Beyond our own experience, we had to acknowledge the critical mass accumulating around web technologies. Companies like Google, Microsoft, and Facebook, not to mention GitHub, are investing &lt;em&gt;incredible&lt;/em&gt; amounts of time, money, and engineering effort in the web as a platform. With Electron, we leverage that investment.&lt;/p&gt;&lt;h2 id=&quot;tradeoffs&quot;&gt;Tradeoffs&lt;/h2&gt;&lt;p&gt;The web isn’t a perfect platform, but native apps aren’t built on perfect platforms either. Rewriting on Electron does mean swapping one set of tradeoffs for another.&lt;/p&gt;&lt;p&gt;Now we can share our logic and UI across all platforms, which is fantastic. But Windows and macOS are different and their users have different expectations. We want to meet those expectations as much as possible, while still sharing the same UI. This manifests in a number of ways, some big and some small.&lt;/p&gt;&lt;p&gt;In the small, some button behaviors vary depending on your platform. On macOS, buttons are &lt;em&gt;Title Case&lt;/em&gt;. On Windows they are &lt;em&gt;Sentence case&lt;/em&gt;. Or on Windows, the default button in dialogs is on the left, where on macOS it’s the right. We enforce the latter convention both &lt;a href=&quot;https://github.com/desktop/desktop/blob/eae0e6f28890c300558bc52a83071ebcab5a7433/app/src/ui/lib/button-group.tsx&quot;&gt;at runtime&lt;/a&gt; and with a &lt;a href=&quot;https://github.com/desktop/desktop/blob/eae0e6f28890c300558bc52a83071ebcab5a7433/tslint-rules/buttonGroupOrderRule.ts&quot;&gt;custom lint rule&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;On macOS, Electron gives us access to the standard app menu bar, but on Windows, the menu support is less than ideal. The menu is shown in the window frame, which doesn’t work with our frameless window design. The built-in menu’s usability is also rough around the edges and doesn’t support the keyboard accessibility Windows users expect.&lt;/p&gt;&lt;p&gt;We &lt;a href=&quot;https://github.com/desktop/desktop/pull/991&quot;&gt;worked hard&lt;/a&gt; to recreate a Windows-appropriate menu in web technologies, complete with access keys and appropriate focus states.&lt;/p&gt;&lt;p&gt;There were times when the web platform or Electron didn’t provide us with the APIs we needed. But in contrast to building a web app, building on Electron meant that we weren’t stuck. We could do something about it.&lt;/p&gt;&lt;p&gt;We &lt;a href=&quot;https://github.com/electron/electron/pull/9099&quot;&gt;added&lt;/a&gt; &lt;a href=&quot;https://github.com/electron/electron/pull/9242&quot;&gt;support&lt;/a&gt; in Electron for importing certificates into the user’s certificate store, using the appropriate platform-specific APIs.&lt;/p&gt;&lt;p&gt;The web’s internationalization support doesn’t provide fine-grained localization information. For example, we can’t format numbers in a &lt;a href=&quot;https://github.com/desktop/desktop/issues/1245&quot;&gt;locale-aware manner&lt;/a&gt;, distinct from the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/NavigatorLanguage/language&quot;&gt;preferred language&lt;/a&gt; defined by the user. Similar to adding the import certificate support, we plan to add better localization support to Electron.&lt;/p&gt;&lt;h2 id=&quot;development-experience&quot;&gt;Development Experience&lt;/h2&gt;&lt;p&gt;If you’re making a native app, your tech stack choices are pretty limited. On the macOS side, you’ll use Xcode, Swift, and AppKit. On Windows, you’ll use Visual Studio, C#, and WPF or UWP. But in the web world, choices abound. React? Angular? CSS? SASS? CSS in JS? Some compile-to-JavaScript language? Browserify? Webpack? Gulp? Grunt? The ecosystem is enormous. The relative openness of the web platform also means developers are able to experiment and innovate, independent from the constraints of any single vendor. This cuts both ways: the array of choices can be &lt;a href=&quot;https://medium.com/@ericclemmons/javascript-fatigue-48d4011b6fc4&quot;&gt;overwhelming&lt;/a&gt;, but it also means you’re more able to pick the right tool for the job.&lt;/p&gt;&lt;p&gt;We were coming from C#, Objective-C, and Swift where static type systems let the compiler watch our back and help us along the way. For us, the question wasn’t &lt;em&gt;if&lt;/em&gt; we’d choose a compile-to-Javascript language but rather which one.&lt;/p&gt;&lt;p&gt;At the same time, one of the big benefits to writing an Electron app is JavaScript itself. It’s the lingua franca of programming. This lowers the barrier of entry for an &lt;a href=&quot;http://github.com/desktop/desktop&quot;&gt;open source project like ours&lt;/a&gt;. So while languages like &lt;a href=&quot;http://elm-lang.org/&quot;&gt;Elm&lt;/a&gt; and &lt;a href=&quot;http://purescript.org/&quot;&gt;PureScript&lt;/a&gt; are interesting and would scratch our static types itch, they were too far outside the mainstream for us to consider.&lt;/p&gt;&lt;p&gt;Our best two options were &lt;a href=&quot;http://flow.org&quot;&gt;Flow&lt;/a&gt; and &lt;a href=&quot;http://typescriptlang.org&quot;&gt;TypeScript&lt;/a&gt;. We landed on TypeScript. At the time we started the project, Flow’s Windows support lagged far behind macOS. For a team like ours where more than half of the engineers live on Windows, this was an instant deal-breaker. Thankfully, TypeScript has been fantastic. Its type system is incredibly expressive, the team at Microsoft moves fast and is responsive to the community, and the community grows every day.&lt;/p&gt;&lt;p&gt;We learned to take the long feedback cycles of native development as a given. Change the code, compile, wait, launch the app, wait, see the change. This doesn’t seem like much, but it adds up. But every minute spent waiting for the compiler to compile or the app to launch is waste. It is time where we could lose our focus, fall out of the flow, and get distracted.&lt;/p&gt;&lt;p&gt;Using web technologies tightens up our feedback cycle. We can tweak designs live, in the app, as it shows real data. Code changes reload in place. Our feedback cycle went from minutes to seconds. It keeps us motivated!&lt;/p&gt;&lt;p&gt;We’ve been working on GitHub Desktop Beta for &lt;a href=&quot;https://github.com/desktop/desktop/commit/7012c51cc29a4ddc5d0f00d3b9763cebab509e4a&quot;&gt;just over a year&lt;/a&gt;. We’re happy with how far we’ve been able to come in that time, but it’s far from done. &lt;a href=&quot;https://desktop.github.com/&quot;&gt;Check it out&lt;/a&gt;, &lt;a href=&quot;https://github.com/desktop/desktop/issues/new&quot;&gt;leave us your feedback&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desktop/desktop&quot;&gt;get involved&lt;/a&gt;!&lt;/p&gt;</content><author><name>William Shepherd</name></author><summary type="html">Today we released the new GitHub Desktop Beta, rewritten on Electron.</summary></entry><entry><title type="html">Enabling DNS split authority with OctoDNS</title><link href="https://githubengineering.com/enabling-split-authority-dns-with-octodns/" rel="alternate" type="text/html" title="Enabling DNS split authority with OctoDNS" /><published>2017-04-27T00:00:00+00:00</published><updated>2017-04-27T00:00:00+00:00</updated><id>https://githubengineering.com/enabling-split-authority-dns-with-octodns</id><content type="html" xml:base="https://githubengineering.com/enabling-split-authority-dns-with-octodns/">&lt;p&gt;Building robust systems involves designing for failure. As Site Reliability Engineers at GitHub, we’re always on the lookout for places where redundancy can help to mitigate problems, and today we’ll be talking about steps we’ve recently taken to shore up how you locate our servers via DNS.&lt;/p&gt;&lt;p&gt;Large &lt;a href=&quot;https://en.wikipedia.org/wiki/Domain_Name_System&quot;&gt;DNS&lt;/a&gt; providers have many levels of redundancy built into their services, but issues will arise causing outages and there are steps that can be taken to lessen their impact. One of the best options available is to split authority for your zones across multiple providers. Enabling split authority is straightforward, you just configure two or more sets of &lt;a href=&quot;https://en.wikipedia.org/wiki/Name_server&quot;&gt;name servers&lt;/a&gt; for your zones in the registrar and DNS requests will be split across the full list. However, the catch is that you now have to keep the records for those zones in sync across multiple providers and depending on the details, that can either be complex to set up or a completely manual process.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dig NS github.com. @a.gtld-servers.net....;; QUESTION SECTION:;github.com. IN NS;; AUTHORITY SECTION:github.com. 172800 IN NS ns4.p16.dynect.net.github.com. 172800 IN NS ns-520.awsdns-01.net.github.com. 172800 IN NS ns1.p16.dynect.net.github.com. 172800 IN NS ns3.p16.dynect.net.github.com. 172800 IN NS ns-421.awsdns-52.com.github.com. 172800 IN NS ns-1283.awsdns-32.org.github.com. 172800 IN NS ns2.p16.dynect.net.github.com. 172800 IN NS ns-1707.awsdns-21.co.uk....&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The above query is asking a &lt;a href=&quot;https://en.wikipedia.org/wiki/Top-level_domain&quot;&gt;TLD name server&lt;/a&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com.&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;NS&lt;/code&gt; records. It returns the values configured in our registrar, in this case four each of two providers. If one of those providers was to experience an outage, the other would hopefully still be available to service requests. We keep our records in sync in further places and can safely change over to them without having to worry about stale or incorrect state.&lt;/p&gt;&lt;p&gt;The last piece of fully configuring split authority is to add all of the name servers as apex &lt;code class=&quot;highlighter-rouge&quot;&gt;NS&lt;/code&gt; records, the root of the zone, in both providers.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ dig NS github.com. @ns1.p16.dynect.net....;; QUESTION SECTION:;github.com. IN NS;; ANSWER SECTION:github.com. 551 IN NS ns1.p16.dynect.net.github.com. 551 IN NS ns2.p16.dynect.net.github.com. 551 IN NS ns-520.awsdns-01.net.github.com. 551 IN NS ns3.p16.dynect.net.github.com. 551 IN NS ns-421.awsdns-52.com.github.com. 551 IN NS ns4.p16.dynect.net.github.com. 551 IN NS ns-1283.awsdns-32.org.github.com. 551 IN NS ns-1707.awsdns-21.co.uk.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At GitHub we have dozens of zones and thousands of records, and while the majority of those aren’t critical enough to require redundancy, we have a fair number that do. We wanted a solution that was able to keep these records in sync in multiple providers and more generally manage all of our DNS records, both internal and external. So today we’re announcing &lt;a href=&quot;https://github.com/github/octodns/&quot;&gt;OctoDNS&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/images/enabling-split-authority-dns-with-octodns/octodns-logo.png&quot; width=&quot;650&quot; height=&quot;256&quot; alt=&quot;octoDNS logo&quot; /&gt;&lt;/p&gt;&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;&lt;p&gt;OctoDNS has allowed us to revamp our DNS workflow. Our zones and records are laid out in config files stored in a Git repo. Changes now use the &lt;a href=&quot;https://guides.github.com/introduction/flow/&quot;&gt;GitHub Flow&lt;/a&gt; and are &lt;a href=&quot;https://githubengineering.com/deploying-branches-to-github-com/&quot;&gt;branch deployed just like the site&lt;/a&gt;. We can even do “noop” deploys to preview what records will be modified by a change. The config files are yaml dictionaries, one per zone, where the top-level keys are record names and the values lay out the ttl, type, and type-specific data. For example, the following config will create the &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; record &lt;code class=&quot;highlighter-rouge&quot;&gt;octodns.github.com.&lt;/code&gt; when included in the zone file &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com.yaml&lt;/code&gt;.&lt;/p&gt;&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;octodns&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1.2.3.4&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1.2.3.5&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The second piece of configuration maps sources of record data to providers. The snippet below tells OctoDNS to load the zone &lt;code class=&quot;highlighter-rouge&quot;&gt;github.com&lt;/code&gt; from the &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; provider and to sync the results to &lt;code class=&quot;highlighter-rouge&quot;&gt;dyn&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;route53&lt;/code&gt;.&lt;/p&gt;&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;zones&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.com.&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;config&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dyn&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;route53&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&quot;synchronizing&quot;&gt;Synchronizing&lt;/h4&gt;&lt;p&gt;Once our configuration is in place OctoDNS can evaluate the current state and build a plan listing the set of changes it would need to match the targets’ state to the source’s. In the example below, &lt;code class=&quot;highlighter-rouge&quot;&gt;octodns.github.com&lt;/code&gt; is a new record so the required action is to create the record in both.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ octodns-sync --config-file=./config/production.yaml...********************************************************************************* github.com.********************************************************************************* route53 (Route53Provider)* Create &amp;lt;ARecord A 60, octodns.github.com., [u'1.2.3.4', '1.2.3.5']&amp;gt;* Summary: Creates=1, Updates=0, Deletes=0, Existing Records=0* dyn (DynProvider)* Create &amp;lt;ARecord A 60, octodns.github.com., [u'1.2.3.4', '1.2.3.5']&amp;gt;* Summary: Creates=1, Updates=0, Deletes=0, Existing Records=0********************************************************************************...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;By default &lt;code class=&quot;highlighter-rouge&quot;&gt;octodns-sync&lt;/code&gt; is in dry-run mode, so no action was taken. Once we’ve reviewed the changes and we’re happy with them, we can run the command again and add the &lt;code class=&quot;highlighter-rouge&quot;&gt;--doit&lt;/code&gt; flag. OctoDNS will run through it’s process and this time continue on to make the necessary changes in Route53 and Dynect so that the new record exists.&lt;/p&gt;&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ octodns-sync --config-file=./config/production.yaml --doit...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point we have consistent record data stored in both providers and can comfortably split our DNS requests across them knowing they’ll be providing the accurate results. While we’re running OctoDNS commands directly above, our internal workflow relies on deploy scripts and chatops. You can find more about that in the &lt;a href=&quot;https://github.com/github/octodns#workflow&quot;&gt;workflow section of the README&lt;/a&gt;.&lt;/p&gt;&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;&lt;p&gt;Split authority is something we feel most sites can benefit from and the hope is that with &lt;a href=&quot;https://github.com/github/octodns/&quot;&gt;OctoDNS&lt;/a&gt;, one of the biggest obstacles to enabling it has been removed. Even if split authority isn’t of interest, OctoDNS may still be worth a look as it brings the benefits of &lt;a href=&quot;https://en.wikipedia.org/wiki/Infrastructure_as_Code&quot;&gt;Infrastructure as Code&lt;/a&gt; to DNS.&lt;/p&gt;&lt;p&gt;Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us. &lt;a href=&quot;https://boards.greenhouse.io/github/jobs/669805#.WPVqJlPyvUI&quot;&gt;Apply Here&lt;/a&gt;&lt;/p&gt;</content><author><name>Ross McFarland</name></author><summary type="html">Building robust systems involves designing for failure. As Site Reliability Engineers at GitHub, we’re always on the lookout for places where redundancy can help to mitigate problems, and today we’ll be talking about steps we’ve recently taken to shore up how you locate our servers via DNS.</summary></entry><entry><title type="html">Open sourcing our Delegated Account Recovery implementation</title><link href="https://githubengineering.com/open-sourcing-our-delegated-recovery-implementation/" rel="alternate" type="text/html" title="Open sourcing our Delegated Account Recovery implementation" /><published>2017-04-14T00:00:00+00:00</published><updated>2017-04-14T00:00:00+00:00</updated><id>https://githubengineering.com/open-sourcing-our-delegated-recovery-implementation</id><content type="html" xml:base="https://githubengineering.com/open-sourcing-our-delegated-recovery-implementation/">&lt;p&gt;In February, we shipped the “&lt;a href=&quot;https://githubengineering.com/recover-accounts-elsewhere/&quot;&gt;Recover Accounts Elsewhere&lt;/a&gt;” feature to help people regain access to their accounts if they lose access to their two-factor device or token. It is an implementation of the &lt;a href=&quot;https://github.com/facebook/DelegatedRecoverySpecification&quot;&gt;Delegated Account Recovery specification&lt;/a&gt; published by Facebook.&lt;/p&gt;&lt;blockquote&gt; &lt;p&gt;GitHub will be open sourcing a Ruby library soon so that the complicated and dangerous code paths will be abstracted away and we will provide a Ruby reference implementation as a guide.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;We’ve released a Ruby library: &lt;a href=&quot;https://github.com/github/darrrr&quot;&gt;Darrrr&lt;/a&gt; (say it with your best Hollywood pirate accent). The library is not coupled with any frameworks and can be used for Rails and non-Rails applications alike. Along with the library, you’ll find a Sinatra application that demonstrates the entire flow.&lt;/p&gt;&lt;p&gt;Additionally, Facebook has released &lt;a href=&quot;https://github.com/facebook/DelegatedRecoveryReferenceImplementation&quot;&gt;SDKs for Java and JavaScript&lt;/a&gt;, including demo applications. They’re also inviting developers to &lt;a href=&quot;https://fb.me/recovery&quot;&gt;join a beta program&lt;/a&gt; for becoming an integrator. You can read more about Facebook’s release on their &lt;a href=&quot;https://www.facebook.com/notes/protect-the-graph/delegated-account-recovery-now-available-in-beta/1875376806035795/&quot;&gt;Protect the Graph page&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Providing open source implementations for common languages was always part of the plan to help ease adoption of the recovery protocol. We hope these libraries and sample implementations lead to more integrations!&lt;/p&gt;</content><author><name>oreoshake</name></author><summary type="html">In February, we shipped the “Recover Accounts Elsewhere” feature to help people regain access to their accounts if they lose access to their two-factor device or token. It is an implementation of the Delegated Account Recovery specification published by Facebook.</summary></entry><entry><title type="html">A formal spec for GitHub Flavored Markdown</title><link href="https://githubengineering.com/a-formal-spec-for-github-markdown/" rel="alternate" type="text/html" title="A formal spec for GitHub Flavored Markdown" /><published>2017-03-14T00:00:00+00:00</published><updated>2017-03-14T00:00:00+00:00</updated><id>https://githubengineering.com/a-formal-spec-for-github-markdown</id><content type="html" xml:base="https://githubengineering.com/a-formal-spec-for-github-markdown/">&lt;p&gt;We are glad we chose Markdown as the markup language for user content atGitHub. It provides a powerful yet straightforward way for users (bothtechnical and non-technical) to write plain text documents that can be renderedrichly as HTML.&lt;/p&gt;&lt;p&gt;Its main limitation, however, is the lack of standarization on the mostambiguous details of the language. Things like how many spaces are needed toindent a line, how many empty lines you need to break between differentelements, and a plethora of other trivial corner cases change betweenimplementations: very similar looking Markdown documents can be rendered aswildly different outputs depending on your Markdown parser of choice.&lt;/p&gt;&lt;p&gt;Five years ago, we started building GitHub’s custom version of Markdown, GFM(GitHub Flavored Markdown) on top of &lt;a href=&quot;https://github.com/vmg/sundown&quot;&gt;Sundown&lt;/a&gt;,a parser which we specifically developed to solve some of the shortcomings ofthe existing Markdown parsers at the time.&lt;/p&gt;&lt;p&gt;Today we’re hoping to improve on this situation by releasing a formalspecification of the syntax for GitHub Flavored Markdown, and its correspondingreference implementation.&lt;/p&gt;&lt;p&gt;This formal specification is based on &lt;a href=&quot;http://commonmark.org/&quot;&gt;CommonMark&lt;/a&gt;, anambitious project to formally specify the Markdown syntax used by many websiteson the internet in a way that reflects its real world usage. CommonMark allowspeople to continue using Markdown the same way they always have, while offeringdevelopers a comprehensive specification and reference implementations tointeroperate and display Markdown in a consistent way between platforms.&lt;/p&gt;&lt;h4 id=&quot;the-specification&quot;&gt;The Specification&lt;/h4&gt;&lt;p&gt;Taking the CommonMark spec and re-engineering our current user content stackaround it is not a trivial endeavour. The main issue we struggled with is thatthe spec (and hence its reference implementations) focuses strictly on thecommon subset of Markdown that is supported by the original Perlimplementation. This does not include some of the extended features that havebeen always available on GitHub. Most notably, support for &lt;em&gt;tables,strikethrough, autolinks and task lists&lt;/em&gt; are missing.&lt;/p&gt;&lt;p&gt;In order to fully specify the version of Markdown we use at GitHub (known asGFM), we had to formally define the syntax and semantics of these features,something which we had never done before. We did this on top of the existingCommonMark spec, taking special care to ensure that our extensions are a strictand optional superset of the original specification.&lt;/p&gt;&lt;p&gt;When reviewing &lt;a href=&quot;https://github.github.com/gfm/&quot;&gt;the GFM spec&lt;/a&gt;, you can clearly tellwhich parts are GFM-specific additions because they’re highlighted as such. Youcan also tell that no parts of the original spec have been modified andtherefore should remain fully compliant with all other implementations.&lt;/p&gt;&lt;h4 id=&quot;the-implementation&quot;&gt;The Implementation&lt;/h4&gt;&lt;p&gt;To ensure that the rendered Markdown in our website is fully compliant with theCommonMark spec, the new backend implementation for GFM parsing on GitHub isbased on &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;, the reference implementation for CommonMark developed by&lt;a href=&quot;https://github.com/jgm&quot;&gt;John MacFarlane&lt;/a&gt; and many other &lt;a href=&quot;https://github.com/jgm/cmark/#authors&quot;&gt;fantasticcontributors&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Just like the spec itself, &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; focuses on parsing a strict subset ofMarkdown, so we had to also implement support for parsing GitHub’s customextensions on top of the existing parser. You can find these changes on our&lt;a href=&quot;https://github.com/github/cmark&quot;&gt;fork of &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;&lt;/a&gt;; in order to track thealways-improving upstream project, we continuously rebase our patches on top ofthe upstream master. Our hope is that once a formal specification for theseextensions is settled, this patchset can be used as a base to upstream thechanges in the original project.&lt;/p&gt;&lt;p&gt;Besides implementing the GFM-specific features in our fork of &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;, we’vealso contributed many changes of general interest to the upstream. The vastmajority of these contributions are focused around performance and security.Our backend renders a massive volume of Markdown documents every day, so ourmain concern lies in ensuring we’re doing these operations as efficiently aspossible, and making sure that it’s not possible to abuse malicious Markdowndocuments to attack our servers.&lt;/p&gt;&lt;p&gt;The first Markdown parsers in C had a terrible security history: it wasfeasible to cause stack overflows (and sometimes even arbitrary code execution)simply by nesting particular Markdown elements sufficiently deep. The &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;implementation, just like our earlier parser Sundown, has been designed fromscratch to be resistant to these attacks. The parsing algorithms and itsAST-based output are thought out to gracefully handle deep recursion and othermalicious document formatting.&lt;/p&gt;&lt;p&gt;The performance side of &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; is a tad more rough: we’ve contributed manyoptimizations upstream based on performance tricks we learnt while implementingSundown, but despite all these changes, the current version of &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; is stillnot faster than Sundown itself: Our benchmarks show it to be between 20% to 30%slower on most documents.&lt;/p&gt;&lt;p&gt;The old optimization adage that &lt;em&gt;“the fastest code is the code that doesn’trun”&lt;/em&gt; applies here: the fact is that &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; just does &lt;em&gt;more things&lt;/em&gt; thanSundown ever did. Amongst other functionality, &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; is UTF8 aware, hasbetter support for references, cleaner interfaces for extension, and mostimportantly: it doesn’t &lt;em&gt;translate&lt;/em&gt; Markdown into HTML, like Sundown did. Itactually generates an AST (Abstract Syntax Tree) out of the source Markdown,which we can transform and eventually render into HTML.&lt;/p&gt;&lt;p&gt;If you consider the amount of HTML parsing that we had to do with Sundown’soriginal implementation (particularly regarding finding user mentions and issuereferences in the documents, inserting task lists, etc), &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;’s AST-basedapproach saves us a tremendous amount of time &lt;em&gt;and&lt;/em&gt; complexity in our usercontent stack. The Markdown AST is an incredibly powerful tool, and well worththe performance cost that &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; pays to generate it.&lt;/p&gt;&lt;h3 id=&quot;the-migration&quot;&gt;The Migration&lt;/h3&gt;&lt;p&gt;Changing our user content stack to be CommonMark compliant is not as simple asswitching the library we use to parse Markdown: the fundamental roadblock weencountered here is that the corner cases that CommonMark specifies (and thatthe original Markdown documentation left ambiguous) could cause some oldMarkdown content to render in unexpected ways.&lt;/p&gt;&lt;p&gt;Through synthetic analysis of GitHub’s massive Markdown corpus, we determinedthat less than 1% of the existing user content would be affected by the newimplementation: we gathered these stats by rendering a large set of Markdowndocuments with both the old (Sundown) and the new (&lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt;, CommonMarkcompliant) libraries, normalizing the resulting HTML, and diffing theirtrees.&lt;/p&gt;&lt;p&gt;1% of documents with minor rendering issues seems like a reasonable tradeoff toswap in a new implementation and reap its benefits, but at GitHub’s scale, 1%is a lot of content, and a lot of affected users. We really don’t want anybodyto check back on an old issue and see that a table that was previouslyrendering as HTML now shows as ASCII – that is bad user experience, eventhough obviously none of the original content was lost.&lt;/p&gt;&lt;p&gt;Because of this, we came up with ways to soften the transition. The first thingwe did was gathering separate statistics on the two different kinds of Markdownuser content we host on the website: comments by the users (such as in Gists,issues, Pull Requests, etc), and Markdown documents inside the Gitrepositories.&lt;/p&gt;&lt;p&gt;There is a fundamental difference between these two kinds of content: the usercomments are stored in our databases, which means their Markdown syntax can benormalized (e.g. by adding or removing whitespace, fixing the indentation, orinserting missing Markdown specifiers until they render properly). The Markdowndocuments stored in Git repositories, however, cannot be touched &lt;em&gt;at all&lt;/em&gt;, astheir contents are hashed as part of Git’s storage model.&lt;/p&gt;&lt;p&gt;Fortunately, we discovered that the vast majority of user content that wasusing complex Markdown features were user comments (particularly Issue bodiesand Pull Request bodies), while the documents stored in Git repositories wererendering properly with both the old and the new renderer in the overwhelmingmajority of cases.&lt;/p&gt;&lt;p&gt;With this in mind, we proceeded to normalize the syntax of the existing usercomments, as to make them render identically in both the old and the newimplementations.&lt;/p&gt;&lt;p&gt;Our approach to translation was rather pragmatic: Our old Markdown parser,Sundown, has always acted as a translator more than a parser. Markdown contentis fed in, and a set of semantic callbacks convert the original Markdowndocument into the corresponding markup for the target language (in our usecase, this was always HTML5). Based on this design approach, we decided to usethe semantic callbacks to make Sundown translate from Markdown toCommonMark-compliant Markdown, instead of HTML.&lt;/p&gt;&lt;p&gt;More than translation, this was effectively a normalization pass, which we hadhigh confidence in because it was performed by the same parser we’ve been usingfor the past 5 years, and hence all the existing documents should be parsedcleanly while keeping their original semantic meaning.&lt;/p&gt;&lt;p&gt;Once we updated Sundown to normalize input documents and sufficiently testedit, we were ready to start the transition process. The first step of theprocess was flipping the switch on the new &lt;code class=&quot;highlighter-rouge&quot;&gt;cmark&lt;/code&gt; implementation for all newuser content, as to ensure that we had a finite cut-off point to finish thetransition at. We actually enabled CommonMark for all &lt;strong&gt;new&lt;/strong&gt; user comments inthe website several months ago, with barely anybody noticing – this is atestament to the CommonMark team’s fantastic job at formally specifying theMarkdown language in a way that is representative of its real world usage.&lt;/p&gt;&lt;p&gt;In the background, we started a MySQL transition to update in-place thecontents of all Markdown user content. After running each comment through thenormalization process, and before writing it back to the database, we’d renderit with the new implementation and compare the tree to the previousimplementation, as to ensure that the resulting HTML output was visuallyidentical and that user data was never destroyed in any circumstances. All inall, less than 1% of the input documents were modified by the normalizationprocess, matching our expectations and again proving that the CommonMark specreally represents the real-world usage of the language.&lt;/p&gt;&lt;p&gt;The whole process took several days, and the end result was that all theMarkdown user content on the website was updated to conform to the new Markdownstandard while ensuring that the final rendered output was visually identicalto our users.&lt;/p&gt;&lt;h4 id=&quot;the-conclusion&quot;&gt;The Conclusion&lt;/h4&gt;&lt;p&gt;Starting today, we’ve also enabled CommonMark rendering for all the Markdowncontent stored in Git repositories. As explained earlier, no normalization hasbeen performed on the existing documents, as we expect the overwhelming majorityof them to render just fine.&lt;/p&gt;&lt;p&gt;We are really excited to have all the Markdown content in GitHub conform to alive and pragmatic standard, and to be able to provide our users with a &lt;a href=&quot;https://github.github.com/gfm/&quot;&gt;clearand authoritative reference&lt;/a&gt; on how GFM isparsed and rendered.&lt;/p&gt;&lt;p&gt;We also remain committed to following the CommonMark specification as it ironsout any last bugs before a final point release. We hope GitHub.com will befully conformant to the 1.0 spec as soon as it is released.&lt;/p&gt;&lt;p&gt;To wrap up, here are some useful links for those willing to learn more about CommonMarkor implement it on their own applications:&lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;a href=&quot;http://commonmark.org/&quot;&gt;The CommonMark website&lt;/a&gt;, with information on the project.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://talk.commonmark.org/&quot;&gt;The CommonMark discussion forum&lt;/a&gt;, where questions and changes to the specification can be proposed.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;http://spec.commonmark.org/&quot;&gt;The CommonMark specification&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/jgm/cmark/&quot;&gt;The reference C Implementation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/github/cmark/&quot;&gt;Our fork with support for all GFM extensions&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.github.com/gfm/&quot;&gt;The GFM specification&lt;/a&gt;, based on the original spec.&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/jgm/CommonMark/wiki/List-of-CommonMark-Implementations&quot;&gt;A list of CommonMark implementations in many programming languages&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</content><author><name>Vicent Martí</name></author><summary type="html">We are glad we chose Markdown as the markup language for user content at GitHub. It provides a powerful yet straightforward way for users (both technical and non-technical) to write plain text documents that can be rendered richly as HTML.</summary></entry></feed>
